{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlpdemyst-vectorization.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitinpunjabi/nlp-demystified/blob/main/notebooks/nlpdemyst_vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F50G99nH112P"
      },
      "source": [
        "# Natural Language Processing Demystified | Vectorization\n",
        "!!course website!!<br>\n",
        "https://github.com/nitinpunjabi/nlp-demystified"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9x6fL6L3zsb"
      },
      "source": [
        "### spaCy upgrade and package installation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88uW0zDh4BkP"
      },
      "source": [
        "At the time this notebook was created, spaCy had recently released 3.0 which was a major upgrade. Colab however was still using version 2.* by default. So the first step is to upgrade spaCy and download a statistical model for English.\n",
        "<br><br>\n",
        "**IMPORTANT**<br>\n",
        "If you're running this for free in the cloud rather than using a paid tier or using a local Jupyter server on your machine, then the notebook will *timeout* after a period of inactivity. If that happens and you don't reconnect in time, you will need to upgrade spaCy again and reinstall the requisite statistical packages.\n",
        "<br><br>\n",
        "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
        "https://research.google.com/colaboratory/local-runtimes.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THBGyQba4Bcm"
      },
      "source": [
        "!pip install -U spacy==3.*\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TK2JoEf24BUH"
      },
      "source": [
        "!python -m spacy info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t81VT9JboTzt"
      },
      "source": [
        "# Basic Bag-of-Words (BOW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1IVdG29wyJ7"
      },
      "source": [
        "## Plain frequency BOW"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fwfWQDVyJpY"
      },
      "source": [
        "# A corpus of sentences.\n",
        "corpus = [\n",
        "  \"Red Bull drops hint on F1 engine.\",\n",
        "  \"Honda exits F1, leaving F1 partner Red Bull.\",\n",
        "  \"Hamilton eyes record eighth F1 title.\",\n",
        "  \"Aston Martin announces sponsor.\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILvS020Zzm6F"
      },
      "source": [
        "We want to build a bag-of-words (BOW) representation of our corpus. Based on what you now know from the lesson, you can probably do this from scratch using dictionaries and lists (and maybe that's a good exercise). Fortunately, there are robust libraries which make it easy.\n",
        "\n",
        "We can use the scikit-learn **CountVectorizer** which takes a collection of text documents and creates a matrix of token counts:<br>\n",
        "https://scikit-learn.org/stable/index.html<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRhJPxbHwuj_"
      },
      "source": [
        "# Instantiate a CountVectorizer object.\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAphZMVPBX9P"
      },
      "source": [
        "The **fit_transform** method does two things:\n",
        "1. It learns a vocabulary dictionary from the corpus.\n",
        "2. It returns a matrix where each row represents a document and each column represents a token (term).<br>\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit_transform\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5wi4_C7BAWv"
      },
      "source": [
        "bow = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Bp1XNcF1FQ"
      },
      "source": [
        "We can take a look at the features and vocabulary dictionary. Notice the **CountVectorizer** took care of tokenization for us. It also removed punctuation and lower-cased everything."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQbqvLgVF8B7"
      },
      "source": [
        "# View features (tokens).\n",
        "print(vectorizer.get_feature_names())\n",
        "\n",
        "# View vocabulary dictionary.\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dmNUkZeExam"
      },
      "source": [
        "Specifically, the **CountVectorizer** generates a sparse matrix using an efficient, compressed representation. The sparse matrix object includes a number of useful methods. See docs for details:<br>\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lug2-xnAExsb"
      },
      "source": [
        "print(type(bow))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bywJ0XnGKPQ"
      },
      "source": [
        "If we look at the raw structure, we'll see tuples where the first element represents the document, and the second element a token ID. It's then followed by a count of that token. So in the second document (index 1), token 8 (\"f1\") occurs twice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At6Gt4bsEx2D"
      },
      "source": [
        "print(bow)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv1N1Io2EyAb"
      },
      "source": [
        "Before we explore further, we want to make a few modifications.\n",
        "1. What if we want to use another tokenizer like spaCy's?\n",
        "2. Instead of frequency, what if we want to have a binary BOW?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRgIHkzUVJtk"
      },
      "source": [
        "## Binary BOW with custom tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tof1PBgqEy1D"
      },
      "source": [
        "**CountVectorizer** supports using a custom tokenizer. For every document, it will call your tokenizer and expect a list of tokens returned. We'll create a simple callback below which has spaCy tokenize and filter tokens, and then return them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcCLawrWEzC7"
      },
      "source": [
        "# As usual, we start by importing spaCy and loading a statistical model.\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a tokenizer callback using spaCy under the hood. Here, we tokenize \n",
        "# the passed-in text and return the tokens, filtering out punctuation.\n",
        "def spacy_tokenizer(doc):\n",
        "  return [t.text for t in nlp(doc) if not t.is_punct]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drEe1Lv_OScv"
      },
      "source": [
        "This time, we instantiate **CountVectorizer** with our custom tokenizer (*spacy_tokenizer*), turn off case-folding, and also set the *binary* parameter to True so we simply get 1s and 0s marking token presence rather than token frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YREyWzaA-rT"
      },
      "source": [
        "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True)\n",
        "bow = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jDKQkZUOysa"
      },
      "source": [
        "Looking at the resulting feature names and vocabulary dictionary, we can see our *spacy_tokenizer* being used. If you're not convinced, you can remove the punctuation filtering in our tokenizer and rerun the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4x6RBqTGq302"
      },
      "source": [
        "print(vectorizer.get_feature_names())\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFpQbdA-R3FI"
      },
      "source": [
        "To get a dense array representation of our sparse matrix, use *toarray*.<br>\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.toarray.html#scipy.sparse.csr_matrix.toarray\n",
        "\n",
        "We can also index and slice into the sparse matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yGr36aP9GCr"
      },
      "source": [
        "print('A dense representation like we saw in the lesson.')\n",
        "print(bow.toarray())\n",
        "print()\n",
        "print('Indexing and slicing.')\n",
        "print(bow[0])\n",
        "print()\n",
        "print(bow[0:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF0NVhdEUR1r"
      },
      "source": [
        "## Cosine Similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leI1VuDVVP4W"
      },
      "source": [
        "Writing your own cosine similarity function is straight-forward using numpy (left as an exercise). There are multiple ways to do it using scipy.\n",
        "<br><br>\n",
        "One way is using the **spatial** package, which is a collection of spatial algorithms and data structures. It has a method to calculate cosine *distance*. To get the cosine *similarity*, we have to substract the distance from 1.<br>\n",
        "https://docs.scipy.org/doc/scipy/reference/spatial.html<br>\n",
        "https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html#scipy.spatial.distance.cosine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOQQ50IgXQfH"
      },
      "source": [
        "# The cosine method expects array_like inputs, so we need to generate\n",
        "# arrays from our sparse matrix.\n",
        "from scipy import spatial\n",
        "doc1_vs_doc2 = 1 - spatial.distance.cosine(bow[0].toarray(), bow[1].toarray())\n",
        "doc1_vs_doc3 = 1 - spatial.distance.cosine(bow[0].toarray(), bow[2].toarray())\n",
        "doc1_vs_doc4 = 1 - spatial.distance.cosine(bow[0].toarray(), bow[3].toarray())\n",
        "\n",
        "print(corpus)\n",
        "\n",
        "print('Doc 1 vs Doc 2: {}'.format(doc1_vs_doc2))\n",
        "print('Doc 1 vs Doc 3: {}'.format(doc1_vs_doc3))\n",
        "print('Doc 1 vs Doc 4: {}'.format(doc1_vs_doc4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SRDwr2gYD04"
      },
      "source": [
        "Another approach is using scikit-learn's **cosine_similarity** which computes the metric between multiple vectors. Here, we pass it our BOW and get a matrix of cosine similarities between each document.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwwP8-jtchSI"
      },
      "source": [
        "# cosine_similarity can take either array-likes or sparse matrices.\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "print(cosine_similarity(bow))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I96W6qDVdDnY"
      },
      "source": [
        "## N-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3E_hN5Ddyae"
      },
      "source": [
        "**CountVectorizer** includes an *ngram_range* parameter to generate different n-grams. n_gram range is specified using a minimum and maximum range. By default, n_gram range is set to (1, 1) which generates unigrams. Setting it to (1, 2) generates both unigrams and bigrams."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZooyyRleHXe"
      },
      "source": [
        "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True, ngram_range=(1,2))\n",
        "bigrams = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "print('Number of features: {}'.format(len(vectorizer.get_feature_names())))\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvtmi3negc0G"
      },
      "source": [
        "# Setting n_gram range to (2, 2) generates only bigrams. \n",
        "vectorizer = CountVectorizer(tokenizer=spacy_tokenizer, lowercase=False, binary=True, ngram_range=(2,2))\n",
        "bigrams = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7e40ZAKhQmm"
      },
      "source": [
        "## Basic Bag-of-Words Exercises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbdMO0bZjROn"
      },
      "source": [
        "#\n",
        "# EXERCISE: Create a spacy_tokenizer callback which takes a string and returns\n",
        "# a list of tokens (each token's text) with punctuation filtered out.\n",
        "#\n",
        "corpus = [\n",
        "  \"Students use their GPS-enabled cellphones to take birdview photographs of a land in order to find specific danger points such as rubbish heaps.\",\n",
        "  \"Teenagers are enthusiastic about taking aerial photograph in order to study their neighbourhood.\",\n",
        "  \"Aerial photography is a great way to identify terrestrial features that arenâ€™t visible from the ground level, such as lake contours or river paths.\",\n",
        "  \"During the early days of digital SLRs, Canon was pretty much the undisputed leader in CMOS image sensor technology.\",\n",
        "  \"Syrian President Bashar al-Assad tells the US it will 'pay the price' if it strikes against Syria.\"\n",
        "]\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def spacy_tokenizer(doc):\n",
        "  pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjBJUUpcBWp2"
      },
      "source": [
        "#\n",
        "# EXERCISE: Initialize a CountVectorizer object and set it to use\n",
        "# your spacy_tokenizer with lower-casing off and to create a binary BOW.\n",
        "#\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Instantiate a CountVectorizer object.\n",
        "\n",
        "\n",
        "# Create a binary BOW from the corpus using your CountVectorizer.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os3tPj5nmRLw"
      },
      "source": [
        "#\n",
        "# The string below is a whole paragraph. We want to create another \n",
        "# binary BOW but using the vocabulary of our *current* CountVectorizer. This means\n",
        "# that words in this paragraph which AREN'T already in the vocabulary won't be\n",
        "# represented. This is to illustrate how BOW can't handle out-of-vocabulary words\n",
        "# unless you rebuild your whole vocabulary. Still, we'll see that if there's\n",
        "# enough overlapping vocabulary, some similarity can still be picked up.\n",
        "#\n",
        "# Note that we call 'transform' only instead of 'fit_transform' because the \n",
        "# fit step (i.e. vocabulary build) is already done and we don't want to re-fit here.\n",
        "#\n",
        "s = [\"Teenagers take aerial shots of their neighbourhood using digital cameras sitting in old bottles which are launched via kites - a common toy for children living in the favelas. They then use GPS-enabled smartphones to take pictures of specific danger points - such as rubbish heaps, which can become a breeding ground for mosquitoes carrying dengue fever.\"]\n",
        "new_bow = vectorizer.transform(s)\n",
        "\n",
        "#\n",
        "# EXERCISE: using the pairwise cosine_similarity method from sklearn, \n",
        "# calculate the similarities between each document from the corpus against \n",
        "# this new document (new_bow). HINT: You can pass two parameters to \n",
        "# cosine_similarity in this case. See the docs:\n",
        "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html#scipy.spatial.distance.cosine\n",
        "#\n",
        "# Which document is the most similar? Which is the least similar? Do the results make sense\n",
        "# based on what you see?\n",
        "#\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXThYmDiwMmR"
      },
      "source": [
        "#\n",
        "# EXERCISE: Implement your own cosine similarity method using numpy.\n",
        "# It should take two numpy arrays and output the similarity metric.\n",
        "# HINTS:\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.dot.html\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\n",
        "#\n",
        "# Verify the similarity between the first document in the corpus and the \n",
        "# paragraph is the same as the one you got from using pairwise cosine_similarity.\n",
        "#\n",
        "import numpy as np\n",
        "def cos_sim(a, b):\n",
        "  pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghlqn6l-dal4"
      },
      "source": [
        "#\n",
        "# EXERCISE: In spacy_tokenizer, instead of returning the plain text,\n",
        "# return the lemma_ attribute instead. How do the cosine similarity \n",
        "# results differ? What if you filter out stop words as well?\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnC_i4oH2ARW"
      },
      "source": [
        "# TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmcTBtSx-XqZ"
      },
      "source": [
        "## Fetching datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYkq3i7_-qhQ"
      },
      "source": [
        "This time around, rather than using a short toy corpus, let's use a larger dataset. scikit-learn has a **datasets** module with utilties to load datasets of your own as well as fetch popular reference datasets online.<br>\n",
        "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets\n",
        "<br><br>\n",
        "We'll use the **20 newsgroups** dataset, which is a collection of 18,000 newsgroup posts across 20 topics.<br>\n",
        "https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset\n",
        "<br><br>\n",
        "List of datasets available:<br>\n",
        "https://scikit-learn.org/stable/datasets.html#datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYjxqxVBBINV"
      },
      "source": [
        "The **datasets** module includes fetchers for each dataset in scikit-learn. For our purposes, we'll fetch only the posts from the *sci.space* topic, and skip on headers, footers, and quoting of other posts.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups\n",
        "<br><br>\n",
        "By default, the fetcher retrieves the *training* subset of the data only. If you don't know what that means, it'll become clear later in the course when we discuss modelling. For now, it doesn't matter for our purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9to6gQNCGiN"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "corpus = fetch_20newsgroups(categories=['sci.space'], remove=('headers', 'footers', 'quotes'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W989GHQxDvTW"
      },
      "source": [
        "We get back a **Bunch** container object containing the data as well as other information.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html\n",
        "<br><br>\n",
        "The actual posts are accessed through the *data* attribute and is a list of strings, each one representing a post."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POGdVmdIDuCK"
      },
      "source": [
        "print(type(corpus))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6AgmbL0ES9I"
      },
      "source": [
        "# Number of posts in our dataset.\n",
        "len(corpus.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAjM4uNDEXGf"
      },
      "source": [
        "# View first two posts.\n",
        "corpus.data[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH99M6cxCpsz"
      },
      "source": [
        "## Creating TF-IDF features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtnQX-wWDhGh"
      },
      "source": [
        "# Like before, if we want to use spaCy's tokenizer, we need \n",
        "# to create a callback. Remember to upgrade spaCy if you need \n",
        "# to (refer to beginnning of file for commentary and instructions).\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# We don't need named-entity recognition nor dependency parsing for\n",
        "# this so these components are disabled. This will speed up the \n",
        "# pipeline. We do need part-of-speech tagging however.\n",
        "unwanted_pipes = [\"ner\", \"parser\"]\n",
        "\n",
        "# For this exercise, we'll remove punctuation and spaces (which\n",
        "# includes newlines), filter for tokens consisting of alphabetic\n",
        "# characters, and return the lemma.\n",
        "def spacy_tokenizer(doc):\n",
        "  with nlp.disable_pipes(*unwanted_pipes):\n",
        "    return [t.lemma_ for t in nlp(doc) if \\\n",
        "            not t.is_punct and \\\n",
        "            not t.is_space and \\\n",
        "            t.is_alpha]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il-0gY9LEiNv"
      },
      "source": [
        "Like the classes to create raw frequency and binary bag-of-words vectors, scikit-learn includes a similar class called **TfidfVectorizer** to create TF-IDF vectors from a corpus.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "<br><br>\n",
        "The usage pattern is similar in that we call *fit_transform* on the corpus which generates the vocabulary dictionary (fit step), and generates the TF-IDF vectors (transform set)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhmWgiZXpUto"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shj6BS0BN6FU"
      },
      "source": [
        "%%time\n",
        "\n",
        "# Use the default settings of TfidfVectorizer.\n",
        "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
        "features = vectorizer.fit_transform(corpus.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ9w4gh9sobB"
      },
      "source": [
        "# The number of unique tokens.\n",
        "print(len(vectorizer.get_feature_names()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CxmKlPcNRLk"
      },
      "source": [
        "# The dimensions of our feature matrix. X rows (documents) by Y features (tokens).\n",
        "print(features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJwnU8PZNdHU"
      },
      "source": [
        "# What the encoding of the first document looks like in sparse format.\n",
        "print(features[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp7VTwYzONlt"
      },
      "source": [
        "As we mentioned in the slides, there are TF-IDF variations out there and scikit-learn, among other things, adds *smoothing* (adds a one to the numerator and denominator in the IDF component), and normalizes by default. These can be disabled if desired using the *smooth_idf* and *norm* parameters respectively. See here for more information:<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylKLM-IMOwbJ"
      },
      "source": [
        "## Querying the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8oTtCg0QB71"
      },
      "source": [
        "The similarity measuring techniques we learned previously can be used here in the same way. In effect, we can query our data using this sequence:\n",
        "1. *Transform* our query using the same vocabulary from our *fit* step on our corpus.\n",
        "2. Calculate the pairwise cosine similarities between each document in our corpus and our query.\n",
        "3. Sort them in descending order by score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNjEUzqlP6Oy"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Transform the query into a TF-IDF vector.\n",
        "query = [\"lunar orbit\"]\n",
        "query_tfidf = vectorizer.transform(query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEfdfkmpP8Tv"
      },
      "source": [
        "# Calculate the cosine similarities between the query and each document.\n",
        "# We're calling flatten() here becaue cosine_similarity returns a list\n",
        "# of lists and we just want a single list.\n",
        "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skuSFhLxXOMC"
      },
      "source": [
        "Now that we have our list of cosine similarities, let's create a utility function that returns the indices of the top k documents with the highest cosine similarities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0PvqRDpUSYO"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# numpy's argsort() method returns a list *indices* that\n",
        "# would sort an array:\n",
        "# https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
        "#\n",
        "# The sort is ascending, but we want the largest k cosine_similarites\n",
        "# at the bottom of the sort. So we negate k, and get the last k\n",
        "# entries of the indices list in reverse order. There are faster\n",
        "# ways to do this using things like argpartition but this is\n",
        "# more succinct.\n",
        "def top_k(arr, k):\n",
        "  kth_largest = (k + 1) * -1\n",
        "  return np.argsort(arr)[:kth_largest:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFYpEldVUaAG"
      },
      "source": [
        "# So for our query above, these are the top five documents.\n",
        "top_related_indices = top_k(cosine_similarities, 5)\n",
        "print(top_related_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e86P3bQR1ZS"
      },
      "source": [
        "# Let's take a look at their respective cosine similarities.\n",
        "print(cosine_similarities[top_related_indices])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzdyTptURiTQ"
      },
      "source": [
        "# Look at the top match.\n",
        "print(corpus.data[top_related_indices[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQwWXypfR8vh"
      },
      "source": [
        "# Second-best match.\n",
        "print(corpus.data[top_related_indices[1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-5aqUbGSM5J"
      },
      "source": [
        "# Try a different query\n",
        "query = [\"satellite\"]\n",
        "query_tfidf = vectorizer.transform(query)\n",
        "\n",
        "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()\n",
        "top_related_indices = top_k(cosine_similarities, 5)\n",
        "\n",
        "print(top_related_indices)\n",
        "print(cosine_similarities[top_related_indices])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHQtRQIcSbTj"
      },
      "source": [
        "print(corpus.data[top_related_indices[0]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4v5wQ4JaBIh"
      },
      "source": [
        "So here we have the beginnings of a simple search engine but we're a far cry from competing with commercial off-the-shelf search engines, let alone Google. Information Retrieval is a large, rich topic and beyond search, is also key in goals like question-answering.\n",
        "<br>\n",
        "- For each query, we're scanning through our entire corpus, but in practice, you'll want to create an *inverted index*. Search applications such as Elasticsearch do that under the hood.\n",
        "- You'd also want to evaluate the efficacy of your search using metrics like *precision* and *recall*.\n",
        "- Document ranking also tends to be more sophisticated, using different ranking functions like Okapi BM25 rather than simple cosine similarity. With major search engines, ranking also involves hundreds of variables such as what the user searched for previously, what do they tend to click on, where are they physically, and on and on. These variables are part of the \"secret sauce\" and are closely guarded by companies.\n",
        "- Beyond word presence, intent and meaning are playing a larger role.\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak3LXiETfGIY"
      },
      "source": [
        "## TF-IDF Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08nTQB7_fJU0"
      },
      "source": [
        "**EXERCISE**<br>\n",
        "Read up on these concepts we just mentioned if you're curious.<br>\n",
        "\n",
        "https://en.wikipedia.org/wiki/Inverted_index<br>\n",
        "https://en.wikipedia.org/wiki/Precision_and_recall<br>\n",
        "https://en.wikipedia.org/wiki/Okapi_BM25<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz2FCCq1fsjz"
      },
      "source": [
        "#\n",
        "# EXERCISE: fetch multiple topics from the 20 newsgroups\n",
        "# dataset and query them using the approach we followed.\n",
        "# A list of topics can be found here: \n",
        "# https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset\n",
        "# \n",
        "# If you're feeling ambitious, incorporate n-grams or\n",
        "# look at how you can measure precision and recall.\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}