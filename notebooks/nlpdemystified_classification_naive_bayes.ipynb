{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitinpunjabi/nlp-demystified/blob/main/notebooks/nlpdemystified_classification_naive_bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egwknkyvG1-C"
      },
      "source": [
        "# Natural Language Processing Demystified | Classification with Naive Bayes\n",
        "https://nlpdemystified.org<br>\n",
        "https://github.com/nitinpunjabi/nlp-demystified\n",
        "<br><br>\n",
        "Course module for this demo: https://www.nlpdemystified.org/course/naive-bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHp03BS8Hhmp"
      },
      "source": [
        "# spaCy upgrade and package installation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TATDBJisHA_w"
      },
      "source": [
        "At the time this notebook was created, spaCy had newer releases but Colab was still using version 2.x by default. So the first step is to upgrade spaCy and download a statistical model for English.\n",
        "<br><br>\n",
        "**IMPORTANT**<br>\n",
        "If you're running this in the cloud, then the notebook will *timeout* after a period of inactivity. If that happens and you don't reconnect in time, you will need to upgrade spaCy again and reinstall the requisite statistical package(s).\n",
        "<br><br>\n",
        "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
        "https://research.google.com/colaboratory/local-runtimes.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K61NFIfSHAn4"
      },
      "source": [
        "!pip install -U spacy==3.*\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "9AQ6Nyad3kiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozlSs1Tz5f7M"
      },
      "source": [
        "# First pass at building a Naive Bayes model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auwysA2BBfuE"
      },
      "source": [
        "As with our TF-IDF demo, we'll use the **20 newsgroups** dataset, a labelled dataset of 18,000 newsgroup posts across 20 topics.<br>\n",
        "https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset\n",
        "\n",
        "This time around, rather than fetching the posts from only one topic, we'll fetch the entire collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCPNpfOIB8fT"
      },
      "source": [
        "# To build our model, we want the training subset only. The training\n",
        "# subset is what gets downloaded by default but we explicitly\n",
        "# pass the parameter here for clarity.\n",
        "training_corpus = fetch_20newsgroups(subset='train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjPpaJitJ0Zf"
      },
      "source": [
        "print('Training data size: {}'.format(len(training_corpus.data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLrleHWZ6b6n"
      },
      "source": [
        "The training data we downloaded not only includes the posts but also a label (\"target\") for each post representing its topic. The posts are an array of strings while the labels are a corresponding array of numeric labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTiTDsbiDyKW"
      },
      "source": [
        "# These are the possible topics a post can belong to.\n",
        "training_corpus.target_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2CVP3GME8Yo"
      },
      "source": [
        "# These are the labels/targets for each post.\n",
        "print(training_corpus.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPuaD40XD3V-"
      },
      "source": [
        "# The first post along with its corresponding label.\n",
        "print(training_corpus.data[0])\n",
        "\n",
        "first_doc_label = training_corpus.target[0]\n",
        "print('Label for this post: {}'.format(first_doc_label))\n",
        "print('Corresponding topic: {}'.format(training_corpus.target_names[first_doc_label]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCaA_rWSGny2"
      },
      "source": [
        "When starting off with a dataset, it's a good idea to check its distribution. In this case, we can see at a glance this dataset is relatively balanced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRbTT7vtDbes"
      },
      "source": [
        "bins, counts = np.unique(training_corpus.target, return_counts=True)\n",
        "freq_series = pd.Series(counts/len(training_corpus.data))\n",
        "plt.figure(figsize=(12, 8))\n",
        "ax = freq_series.plot(kind='bar')\n",
        "ax.set_xticklabels(bins, rotation=0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akAhiAsuH0Vm"
      },
      "source": [
        "Now that we have our training set, we can split it further into train and validation sets (remember the test set, in this case, is a separate download). Creating a validation set isn't always necessary. If you have a small training set like this one, you can use alternative techniques like cross-validation but we'll show a split here since we talked about it in the model building module. scikit-learn has a module to help us do this.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3rdIx__I7JX"
      },
      "source": [
        "# Shuffle, then split the data into train and validation sets. Set the random_state \n",
        "# to 1 for reproducibility.\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(training_corpus.data, training_corpus.target, train_size=0.8, random_state=1) \n",
        "print('Training data size: {}'.format(len(train_data)))\n",
        "print('Validation data size: {}'.format(len(val_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sg6rGId2K4eb"
      },
      "source": [
        "Now that we have our train-validation split, let's create our spaCy tokenizer. Up to this point, we've been using the **en_core_web_sm** model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4pQw-2KK2MV"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElofxHmW7fP_"
      },
      "source": [
        "By default, it comes up with a preprocessing pipeline with several components enabled. We can view these components through the *pipe_names* attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYATX6AS4k-b"
      },
      "source": [
        "nlp.pipe_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2P0S-Tm7toD"
      },
      "source": [
        "In the previous demos, we individually disabled any component we didn't need. For our first pass at building a Naive Bayes classifier, we'll try tokenizing alone. Nothing else. Since that's the case, it's easier to instantiate a blank pipeline.<br>\n",
        "https://spacy.io/api/top-level#spacy.blank"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AauBdVeA4znZ"
      },
      "source": [
        "nlp = spacy.blank('en')\n",
        "\n",
        "# There should be no pipeline components.\n",
        "nlp.pipe_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1tLv7nF4Za2"
      },
      "source": [
        "# For this exercise, we'll remove punctuation and spaces (which\n",
        "# includes newlines), filter for tokens consisting of alphabetic\n",
        "# characters only, and return the token text.\n",
        "def spacy_tokenizer(doc):\n",
        "  return [t.text for t in nlp(doc) if \\\n",
        "          not t.is_punct and \\\n",
        "          not t.is_space and \\\n",
        "          t.is_alpha]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylefAguOKgVk"
      },
      "source": [
        "We'll vectorize using the **TfidfVectorizer**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpX_QndoCbaz"
      },
      "source": [
        "%%time\n",
        "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
        "train_feature_vects = vectorizer.fit_transform(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWys2B9xOXiq"
      },
      "source": [
        "Scikit-learn includes a multinomial naive bayes classifier.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTo6o5LyYNnq"
      },
      "source": [
        "Calling *fit* on the classifier and passing it the feature vectors and corresponding labels kicks off the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4RouNa1OT_U"
      },
      "source": [
        "# Instantiate a classifier with the default settings.\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(train_feature_vects, train_labels)\n",
        "nb_classifier.get_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUWJuhJlPiHS"
      },
      "source": [
        "Now that we know about the **F1 score** and have a multiclass problem, let's look at the F1 score on the training data. Since the dataset is balanced, accuracy could work here as well but we'll look at F1 since we introduced it. scikit-learn has a module called **metrics** we can leverage. It contains a variety of scoring utilities we can use.<br>\n",
        "https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score<br>\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ3MctXZYB9B"
      },
      "source": [
        "# Get predictions on training set and calculate F1 score.\n",
        "# See documentation above for more details on what \"macro\" means.\n",
        "train_preds = nb_classifier.predict(train_feature_vects)\n",
        "print('F1 score on initial training set: {}'.format(metrics.f1_score(train_labels, train_preds, average='macro')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw76J_P0QJa5"
      },
      "source": [
        "So right off the bat, using simple preprocessing and vectorization, and the default settings on the Naive Bayes classifier, we get a model with a decent F1 score. This looks good, but there's a problem.<br><br>\n",
        "When we downloaded the training data, we also included headers and footers which contain metadata like *subject*, and *email*.<br><br>\n",
        "This can be a problem because these fields may be highly informative, causing the model to predict mostly based on the metadata rather than the post content. But if this metadata isn't available at prediction time in production, then our model is going to perform poorly.\n",
        "<br><br>\n",
        "So let's retrieve the training data again but without the headers, footers, and post quotes this time. Just raw post text. This makes the problem notably harder for reasons we'll see soon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67YsfFTtQcxF"
      },
      "source": [
        "# Remove headers, footers, and quotes from training set and resplit.\n",
        "filtered_training_corpus = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(filtered_training_corpus.data, filtered_training_corpus.target, train_size=0.8, random_state=1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ihNfcfVQcmO"
      },
      "source": [
        "# This is what a data point looks like now. Just plain post text.\n",
        "train_data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNF3e3jzQcZW"
      },
      "source": [
        "# Revectorize our text and retrain our model.\n",
        "%%time\n",
        "train_feature_vects = vectorizer.fit_transform(train_data)\n",
        "nb_classifier.fit(train_feature_vects, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0CJ3wIuQcO3"
      },
      "source": [
        "# Recheck F1 score on training data.\n",
        "train_preds = nb_classifier.predict(train_feature_vects)\n",
        "print('F1 score on filtered training set: {}'.format(metrics.f1_score(train_labels, train_preds, average='macro'))) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEuysjV4QcGO"
      },
      "source": [
        "Now that we've removed metadata, our F1 score has dropped but still seems ok. The next step is to see how well the classifier performs on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnVNZ2WaQb9O"
      },
      "source": [
        "# Vectorize the validation data.\n",
        "%%time\n",
        "val_feature_vects = vectorizer.transform(val_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph8R2E6YUSfG"
      },
      "source": [
        "# Predict and evaluate.\n",
        "val_preds = nb_classifier.predict(val_feature_vects)\n",
        "print('F1 score on filtered validation set: {}'.format(metrics.f1_score(val_labels, val_preds, average='macro')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBCFRGBxQbql"
      },
      "source": [
        "That's quite a drop in F1 score. Because there are 20 classes involved, let's plot a confusion matrix to see what's going on:<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html<br>\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiITJWX3W9cD"
      },
      "source": [
        "# Set the size of the plot.\n",
        "fig, ax = plt.subplots(figsize=(15, 15))\n",
        "\n",
        "# Create the confusion matrix. \n",
        "disp = ConfusionMatrixDisplay.from_estimator(nb_classifier, val_feature_vects, val_labels, normalize='true', display_labels=filtered_training_corpus.target_names, xticks_rotation='vertical', ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnc6wj02ZqYz"
      },
      "source": [
        "Similar to what we saw in the slides, the y-axis represents the true labels and the x-axis represents the predictions. Each square's brightness represents the number of posts assigned to that class. What we ideally want is brightness along the diagonal (top-left to bottom-right) which represent correct predictions, and little to no brightness anywhere else.\n",
        "<br><br>\n",
        "Looking at the confusion matrix above, we can make a few observations:\n",
        "1. The more specific a topic is, the better the prediction result. Hockey and cryptography are good examples. This intuitively makes sense.\n",
        "2. Topics with a lot of word overlap tend to have higher errors. For example, the majority of atheism and religion.misc posts are classified under christianity. In general, the christianity column has a prevalence of brighter squares with misclassified posts from politics.misc, politics.mideast, etc.\n",
        "3. There's a smaller, secondary cluster of errors around the computer-related topics (e.g. posts in electronics being misclassified as hardware).\n",
        "<br><br>\n",
        "Seeing the results of this matrix, at least there are plausible explanations for the discrepancies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8RoMv8yg9p5"
      },
      "source": [
        "Let's take a look at **precision** and **recall** for each label:<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html<br>\n",
        "https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0P_PA6ebjAU"
      },
      "source": [
        "print(metrics.classification_report(val_labels, val_preds, target_names=filtered_training_corpus.target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ppg_HUVee8y7"
      },
      "source": [
        "A few observations:\n",
        "1. Atheism has a perfect precision score but terrible recall, signalling that the model was right when it classified something as under atheism, but missed the vast majority in the corpus. The model didn't classify anything under religion.misc.\n",
        "2. The more specific the topic, the better it tends to do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg_-c_mueeo-"
      },
      "source": [
        "# Improving the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sh4NkAciNdp"
      },
      "source": [
        "Let's try to do better. One thing that's likely an issue is the sheer number of features we have relative to how little data there is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFjW_Ee5e433"
      },
      "source": [
        "print('Training data size: {}'.format(len(train_data)))\n",
        "print('Number of training features: {}'.format(len(train_feature_vects[0].toarray().flatten())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB5wpYxzkdFx"
      },
      "source": [
        "So we can experiment with:\n",
        "1. Removing stop words because topic identification likely depends more on keywords rather than sequences in this case.\n",
        "3. Using the token lemma rather than the text.\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZvOroxYAYtj"
      },
      "source": [
        "We can't get away with the blank pipeline since we need a bunch of components to generate the lemma. So we'll load the **en_core_web_sm** model and disable named-entity recognition and parsing in the tokenizer callback."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuqg4x5z5R6M"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2I72F5peeFR"
      },
      "source": [
        "unwanted_pipes = ['ner', 'parser']\n",
        "\n",
        "# Further remove stop words and take the lemma instead of token text.\n",
        "def spacy_tokenizer(doc):\n",
        "  with nlp.disable_pipes(*unwanted_pipes):\n",
        "    return [t.lemma_ for t in nlp(doc) if \\\n",
        "            not t.is_punct and \\\n",
        "            not t.is_space and \\\n",
        "            not t.is_stop and \\\n",
        "            t.is_alpha]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFFLr6TRjIkS"
      },
      "source": [
        "We need to re-vectorize the training set with the new tokenizer. Because there are certain components enabled, this is going to take longer (a few mins). Take a look at these link for ways to further optimize spaCy's pipeline:<br>\n",
        "https://spacy.io/usage/processing-pipelines#processing<br>\n",
        "https://spacy.io/api/language#pipe<br><br>\n",
        "YouTube video from spaCy on using **nlp.pipe**: [Speed up spaCy pipelines via `nlp.pipe` - spaCy shorts](https://www.youtube.com/watch?v=OoZ-H_8vRnc)<br>\n",
        "Tuning **nlp.pipe**: https://stackoverflow.com/questions/65850018/processing-text-with-spacy-nlp-pipe<br>\n",
        "Passing a list of pre-processed tokens to TfidfVectorizer: https://www.davidsbatista.net/blog/2018/02/28/TfidfVectorizer/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzRMdHOg-Z1c"
      },
      "source": [
        "%%time\n",
        "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
        "train_feature_vects = vectorizer.fit_transform(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBY6WanaBVw5"
      },
      "source": [
        "# Check the number of features now.\n",
        "print('Number of training features: {}'.format(len(train_feature_vects[0].toarray().flatten())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x9EYRCRlIDW"
      },
      "source": [
        "A little better but still not great. Let's retrain our classifier and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykdDS-De9-Cy"
      },
      "source": [
        "nb_classifier.fit(train_feature_vects, train_labels)\n",
        "train_preds = nb_classifier.predict(train_feature_vects)\n",
        "print('Training F1 score with fewer features: {}'.format(metrics.f1_score(train_labels, train_preds, average='macro')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBLUzjod996N"
      },
      "source": [
        "Check classifier performance on validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9EH7UyvliIJ"
      },
      "source": [
        "%%time\n",
        "val_feature_vects = vectorizer.transform(val_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHntKMpPliRZ"
      },
      "source": [
        "val_preds = nb_classifier.predict(val_feature_vects)\n",
        "print('Validation F1 score with fewer features: {}'.format(metrics.f1_score(val_labels, val_preds, average='macro')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMOCM485mPky"
      },
      "source": [
        "We managed to squeeze out a few percentage points. Let's look at the confusion matrix and classification report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH67cCdwliZH"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 15))\n",
        "disp = ConfusionMatrixDisplay.from_estimator(nb_classifier, val_feature_vects, val_labels, normalize='true', display_labels=filtered_training_corpus.target_names, xticks_rotation='vertical', ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7ifQJaSlije"
      },
      "source": [
        "print(metrics.classification_report(val_labels, val_preds, target_names=filtered_training_corpus.target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xlt7kUmmfTB"
      },
      "source": [
        "In the confusion matrix, the squares in the christian column have dimmed, signalling fewer classification errors. And although atheism now classifies better, that topic along with religion.misc remain big sources of overall errors.\n",
        "\n",
        "Let's assume for now that we can't get or generate more data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWyhcbn9mfcL"
      },
      "source": [
        "Next, we can try tuning a hyperparameter on the classifier. For Naive Bayes, we'll adjust the the *alpha* smoothing factor we discussed in the slides. But rather than trying a bunch ourselves, we can use a combination of **Grid Search** and **Cross Validation**.\n",
        "- Grid search involves having the computer try a list of hyperparameter values for us, and returning the best performing value. The list of hyperparameter values to try is supplied by us. Grid search is a basic technique and there are a number of other techniques such as **random search** and **bayesian optimization**.\n",
        "- Cross validation is a way to evaluate machine learning models on limited datasets. It randomly splits the data into k-groups. One group is set aside as the holdout set while the classifier trains a model on the remaining groups. The resulting model is then used on the holdout group and the score recorded. This repeats itself until all groups have been used as a holdout set and an average score returned.\n",
        "\n",
        "Scikit-learn has modules to handle both for us:<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html<br>\n",
        "https://scikit-learn.org/stable/modules/grid_search.html#grid-search<br>\n",
        "https://scikit-learn.org/stable/modules/cross_validation.html<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCSbxiQWpWNg"
      },
      "source": [
        "# The alpha values to try.\n",
        "params = {'alpha': [0.01, 0.1, 0.5, 1.0, 10.0,],}\n",
        "\n",
        "# Instantiate the search with the model we want to try and fit it on the training data.\n",
        "multinomial_nb_grid = model_selection.GridSearchCV(MultinomialNB(), param_grid=params, scoring='f1_macro', n_jobs=-1, cv=5, verbose=5)\n",
        "multinomial_nb_grid.fit(train_feature_vects, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3NsSXcKpky2"
      },
      "source": [
        "The resulting **GridSearchCV** object has a number of attributes you can explore:<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
        "<br><br>\n",
        "We're interested in the best performing parameter value(s)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJwHWedlpWcM"
      },
      "source": [
        "print('Best parameter value(s): {}'.format(multinomial_nb_grid.best_params_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUrc7dbzp27E"
      },
      "source": [
        "You can directly access the best estimator found by the search. Let's try using it on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlF5Ji1vqDq0"
      },
      "source": [
        "best_nb_classifier = multinomial_nb_grid.best_estimator_\n",
        "val_preds = best_nb_classifier.predict(val_feature_vects)\n",
        "print('Validation F1 score with fewer features: {}'.format(metrics.f1_score(val_labels, val_preds, average='macro')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzhjYdM-qYkk"
      },
      "source": [
        "So we got another decent jump after using the the optimal *alpha* value. Let's look at the confusion matrix (using the best estimator so far) and classification report again.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxaQxpAdpWjc"
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 15))\n",
        "disp = ConfusionMatrixDisplay.from_estimator(best_nb_classifier, val_feature_vects, val_labels, normalize='true', display_labels=filtered_training_corpus.target_names, xticks_rotation='vertical', ax=ax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndj-_mlKqnvd"
      },
      "source": [
        "print(metrics.classification_report(val_labels, val_preds, target_names=filtered_training_corpus.target_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsslL_bsqnnB"
      },
      "source": [
        "A few observations from this one:\n",
        "1. Atheism and religion.misc are doing much better though still a source of errors.\n",
        "2. The christian column has dimmed further in the other categories.\n",
        "\n",
        "Given the small data size and the soft borders around various topics, what we have now is probably good enough. A few further ideas to explore:\n",
        "1. Augment the training data with posts from similar subreddits.\n",
        "2. Incorporate n-grams.\n",
        "3. Remove the *misc* categories if your goal allows it.\n",
        "4. Merge a few categories with large overlap together if your goal allows it.\n",
        "5. Use the **CountVectorizer** instead of the **TfidfVectorizer**.\n",
        "6. Play around with adding more stop words after seeing which ones are the most prevalent.\n",
        "7. Play with the min_df, max_df, and max_features in the **TFidfVectorizer**.\n",
        "8. Use a dimensionality reduction technique like Singular Value Decomposition (SVD) or dense word vectors which we'll cover in Part II.\n",
        "9. Try other models: logistic regression, support vector machines, random forests, SGD classifier.\n",
        "\n",
        "My guess is that, aside from merging categories, it'll be hard to do much better than what we have given the nature of the data.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "<br>\n",
        "https://scikit-learn.org/stable/modules/svm.html#svm-classification\n",
        "<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG4KYHvPuUwy"
      },
      "source": [
        "For idea **(6)**, we can use the function below to view the most commonly occurring words in each category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO1vyE_Jqnd9"
      },
      "source": [
        "def show_top_words(classifier, vectorizer, categories, top_n):\n",
        "  feature_names = np.asarray(vectorizer.get_feature_names_out())\n",
        "  for i, category in enumerate(categories):\n",
        "    prob_sorted = classifier.feature_log_prob_[i, :].argsort()[::-1]\n",
        "    print(\"%s: %s\" % (category, \" \".join(feature_names[prob_sorted[:top_n]])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzrTHDDktms8"
      },
      "source": [
        "show_top_words(best_nb_classifier, vectorizer, filtered_training_corpus.target_names, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a sanity check, we can use scikit-learns **DummyClassifier** which can make predictions using strategies such as \"just guess the most frequently occurring class\" or \"make random guesses\".<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html"
      ],
      "metadata": {
        "id": "LtexSjOp_4wj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1e28pieaexT"
      },
      "source": [
        "# Train a dummy classifier which just guesses the most frequent class.\n",
        "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
        "dummy_clf.fit(train_feature_vects, train_labels)\n",
        "dummy_clf.score(val_feature_vects, val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g53QB8nGbeoB"
      },
      "source": [
        "# Train a dummy classifier which just guesses a class randomly.\n",
        "dummy_clf = DummyClassifier(strategy=\"uniform\")\n",
        "dummy_clf.fit(train_feature_vects, train_labels)\n",
        "dummy_clf.score(val_feature_vects, val_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXN_3sltvNWV"
      },
      "source": [
        "# Creating the final Naive Bayes classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBtpCqa8t5GZ"
      },
      "source": [
        "Let's train the classifier we'll use on the test set. We'll use the entire original training set (including validation data) and the ideal *alpha* param.\n",
        "<br>\n",
        "We'll also use scikit-learn's **Pipeline** to specify a series of transformation and training steps so we can vectorize and fit a model with one call. Creating a few of these pipelines can help speed up your development and stay organized:<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrj1BURWw3AP"
      },
      "source": [
        "text_classifier = Pipeline([\n",
        "  ('vectorizer', TfidfVectorizer(tokenizer=spacy_tokenizer)),\n",
        "  ('classifier', MultinomialNB(alpha=0.01))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeznMt1Fze3Z"
      },
      "source": [
        "%%time\n",
        "text_classifier.fit(filtered_training_corpus.data, filtered_training_corpus.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWvrtPo90u_B"
      },
      "source": [
        "Download the 20 newsgroups *test* dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gtr4vs4du5Hl"
      },
      "source": [
        "filtered_test_corpus = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxWYFXwvNzPx"
      },
      "source": [
        "We can now pass the raw test data directly to the classifier."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "test_preds = text_classifier.predict(filtered_test_corpus.data)"
      ],
      "metadata": {
        "id": "3eDucMkzj5Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "fig, ax = plt.subplots(figsize=(15, 15))\n",
        "ConfusionMatrixDisplay.from_predictions(filtered_test_corpus.target, test_preds, normalize='true', display_labels=filtered_test_corpus.target_names, xticks_rotation='vertical', ax=ax)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k8ThbeHJj5IM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeKTyl-W07Wi"
      },
      "source": [
        "Looking at the confusion matrix for test data classification, we see there are still a few brighter clusters around the soft politics/religion area as well as the finer-grained computer-related topics which drag the overall accuracy down. This is reflected in the classification report as well. Overall, the other topics look ok given the data we have.\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.classification_report(filtered_test_corpus.target, test_preds, target_names=filtered_test_corpus.target_names))"
      ],
      "metadata": {
        "id": "gQ5m0fEkj5Ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now leverage our pipeline to classify new documents on the fly now.\n",
        "<br><br>\n",
        "The function below takes a classifier, a document to classify, and an optional set of labels. It returns a tuple of the most probable class and its probability. With this information, you can choose a probability threshold over which to accept a classification. If it falls below the threshold, perhaps you can classify it in some default bucket or pass it to a human, or to another classifier downstream. You could also require a minimum string length for classification along with other conditions.\n",
        "<br><br>\n"
      ],
      "metadata": {
        "id": "csncNP4BmUd_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ2lYTdW3HXP"
      },
      "source": [
        "def classify_text(clf, doc, labels=None):\n",
        "  probas = clf.predict_proba([doc]).flatten()\n",
        "  max_proba_idx = np.argmax(probas)\n",
        "  \n",
        "  if labels:\n",
        "    most_proba_class = labels[max_proba_idx]\n",
        "  else:\n",
        "    most_proba_class = max_proba_idx\n",
        "\n",
        "  return (most_proba_class, probas[max_proba_idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icAHgqa2PDIk"
      },
      "source": [
        "The strings below were taken at random from subreddits that have corresponding topics (e.g. r/space, r/cars, etc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-YZ3euf4hg1"
      },
      "source": [
        "# Post from r/medicine.\n",
        "s = \"Hello everyone so am doing my thesis on Ischemic heart disease have been using online articles and textbooks mostly Harrisons internal med. could u recommended me some source specifically books where i can get more about in depth knowledge on IHD.\"\n",
        "classify_text(text_classifier, s, filtered_test_corpus.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGZJYEz8Pb65"
      },
      "source": [
        "# Post from r/space.\n",
        "s = \"First evidence that water can be created on the lunar surface by Earth's magnetosphere. Particles from Earth can seed the moon with water, implying that other planets could also contribute water to their satellites.\"\n",
        "classify_text(text_classifier, s, filtered_test_corpus.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3_ryq6VPdza"
      },
      "source": [
        "# Post from r/cars.\n",
        "s = \"New Toyota 86 Launch Reportedly Delayed to 2022, CEO Doesn't Want a Subaru Copy\"\n",
        "classify_text(text_classifier, s, filtered_test_corpus.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mOI3OSN4k0n"
      },
      "source": [
        "# Post from r/electronics.\n",
        "s = \"My First Ever Homemade PCB. My SMD Soldering Skills Aren't Great, But I'm Quite Proud of it.\"\n",
        "classify_text(text_classifier, s, filtered_test_corpus.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "012m-1j1Q_Mg"
      },
      "source": [
        "These are a few made-up statements with low probability which could belong to anything. In these situations, they can be dealt with as special cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ct2AiGH5RrJ"
      },
      "source": [
        "s = \"I don't know if that's a good idea.\"\n",
        "classify_text(text_classifier, s, filtered_test_corpus.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYppCUPARU-N"
      },
      "source": [
        "s = \"Hold on for dear life.\"\n",
        "classify_text(text_classifier, s, filtered_test_corpus.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzxyKMHzegBI"
      },
      "source": [
        "**Note:**<br>\n",
        "Keep in mind that Naive Bayes is good at returning the most probable class but is regarded as a poor estimator because of its naive assumption of independence (i.e. the actual probability values aren't very reliable)."
      ]
    }
  ]
}