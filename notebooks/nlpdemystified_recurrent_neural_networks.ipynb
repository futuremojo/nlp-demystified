{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlpdemystified-recurrent-neural-networks.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jY9ICurtUoDs"
      ],
      "authorship_tag": "ABX9TyN5X5Ha/jROaDrDXFFGMSWr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/futuremojo/nlp-demystified/blob/main/notebooks/nlpdemystified_recurrent_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing Demystified | Recurrent Neural Networks\n",
        "https://nlpdemystified.org<br>\n",
        "https://github.com/futuremojo/nlp-demystified<br><br>\n",
        "\n",
        "\n",
        "Course module for this demo: https://www.nlpdemystified.org/course/recurrent-neural-networks"
      ],
      "metadata": {
        "id": "KmEyadzTtGxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**<br>\n",
        "Enable **GPU acceleration** by going to *Runtime > Change Runtime Type*. Keep in mind that, on certain tiers, you're not guaranteed GPU access depending on usage history and current load.\n",
        "<br><br>\n",
        "Also, if you're running this in the cloud rather than a local Jupyter server on your machine, then the notebook will *timeout* after a period of inactivity.\n",
        "<br><br>\n",
        "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
        "https://research.google.com/colaboratory/local-runtimes.html"
      ],
      "metadata": {
        "id": "91LZr9ogtHi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from nltk.corpus import treebank, brown, conll2000\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "GmJ9N1blcHXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part-of-Speech Tagging with a Bidirectional LSTM"
      ],
      "metadata": {
        "id": "PVbpx-8_tSrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's difficult to find free sequence labelling datasets because they're so labour-intensive to create.\n",
        "<br><br>\n",
        "Fortunately, **Natural Language Toolkit (NLTK)** includes enough free sets of labelled corpora for our purposes. NLTK also provides them in a convenient uniform format.<br>\n",
        "https://www.nltk.org/index.html<br>\n",
        "https://www.nltk.org/nltk_data/<br>\n",
        "<br>\n",
        "We'll use the Treebank, Brown, and CONLL-2000 datasets. "
      ],
      "metadata": {
        "id": "1LVEGU9cta45"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TawHhQnsENGX"
      },
      "outputs": [],
      "source": [
        "nltk.download('treebank')\n",
        "nltk.download('brown')\n",
        "nltk.download('conll2000')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In their original form, the datasets use different part-of-speech (PoS) tag sets. We need to ensure they all use the same tagset, so we'll download a simplified set called the *universal_tagset* from NLTK.<br>\n",
        "\n",
        "See Section 2.3 here for a list of tags: https://www.nltk.org/book/ch05.html"
      ],
      "metadata": {
        "id": "W1AMD2ojvtSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('universal_tagset')"
      ],
      "metadata": {
        "id": "3Ska6B_5vs0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll then retrieve the tagged sentences from each dataset, taking care to specify they should use the *universal tagset* we just downloaded. We'll then combine them into one collection."
      ],
      "metadata": {
        "id": "yzvREp9EwNld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download all PoS-tagged sentences and place them in one list.\n",
        "tagged_sentences = treebank.tagged_sents(tagset='universal') +\\\n",
        "                   brown.tagged_sents(tagset='universal') +\\\n",
        "                   conll2000.tagged_sents(tagset='universal')\n",
        "\n",
        "print(tagged_sentences[0])\n",
        "print(f\"Dataset size: {len(tagged_sentences)}\")"
      ],
      "metadata": {
        "id": "srteMIhDEbRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each tagged sentence is actually a list of word-tag tuples (bear in mind that NLTK's universal tagset is a reduced tagset so items such as *proper nouns* are simply tagged as *nouns*).<br>\n",
        "\n",
        "Our model is going to take in a sequence of words, and output a sequence of PoS tags, so we need to separate the words from the tags in our dataset. The tag sequences will serve as our training labels."
      ],
      "metadata": {
        "id": "vY9upptpw6fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, sentence_tags = [], []\n",
        "\n",
        "for s in tagged_sentences:\n",
        "  sentence, tags = zip(*s)\n",
        "  sentences.append(list(sentence))\n",
        "  sentence_tags.append(list(tags))"
      ],
      "metadata": {
        "id": "L7unpKPbNOUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The sentences and their respective tags are now in separate lists."
      ],
      "metadata": {
        "id": "pzK70wbrH71p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[0])\n",
        "print(sentence_tags[0])"
      ],
      "metadata": {
        "id": "2Im5-zeiE1jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sentences), len(sentence_tags))"
      ],
      "metadata": {
        "id": "fyYJQufRi_4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create train/validation/test splits. This time, we don't have a separate test set so we'll call *train_test_split* twice.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ],
      "metadata": {
        "id": "sL7Voept0lEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.75\n",
        "validation_ratio = 0.15\n",
        "test_ratio = 0.10\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(sentences, sentence_tags, \n",
        "                                                    test_size=1 - train_ratio, \n",
        "                                                    random_state=1)\n",
        "\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, \n",
        "                                                test_size=test_ratio/(test_ratio + validation_ratio), \n",
        "                                                random_state=1)"
      ],
      "metadata": {
        "id": "Dqmencun067o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train), len(y_train))\n",
        "print(len(x_val), len(y_val))\n",
        "print(len(x_test), len(y_test))"
      ],
      "metadata": {
        "id": "dRPQyAgfR5sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our datasets preprocessed, the next step is to vectorize. If you watched the demo on **Word Vectors**, the next few steps should look familiar.<br>\n",
        "https://www.nlpdemystified.org/course/word-vectors<br>\n",
        "https://colab.research.google.com/github/futuremojo/nlp-demystified/blob/main/notebooks/nlpdemystified_word_vectors.ipynb"
      ],
      "metadata": {
        "id": "joyhlIqixpCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we need to create a tokenizer for the sentences and *fit* it to the training dataset to create a vocabulary. We'll just use the default tokenizer settings which applies some light filtering, lowers the case, and separates on spaces. We'll also supply an out-of-vocabulary token (\\<OOV\\>) in case the tokenizer encounters words during testing/inference which it doesn't during training.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"
      ],
      "metadata": {
        "id": "kT5VFgpRxlzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokenizer = keras.preprocessing.text.Tokenizer(oov_token='<OOV>')"
      ],
      "metadata": {
        "id": "CV1gT-GmOOI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokenizer.fit_on_texts(x_train)"
      ],
      "metadata": {
        "id": "XMmMUDogOPoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Vocabulary size: {len(sentence_tokenizer.word_index)}\")"
      ],
      "metadata": {
        "id": "ziy90iSwORAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to create *another* tokenizer for the tags since our labels are also sequences. This time, we won't need an OOV token because there are only a handful of tags and, in this case, they'll all be encountered during training."
      ],
      "metadata": {
        "id": "5icc4yz_yWRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer = keras.preprocessing.text.Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(y_train)"
      ],
      "metadata": {
        "id": "3H571o7bOSQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of PoS tags: {len(tag_tokenizer.word_index)}\\n\")\n",
        "tag_tokenizer.get_config()"
      ],
      "metadata": {
        "id": "9fDtT7_YOhoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The set of universal PoS tags.\n",
        "tag_tokenizer.word_index"
      ],
      "metadata": {
        "id": "YJrIrOwzyddJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we need to vectorize our sentences and corresponding tags. As we did in the demo on [Word Vectors](https://github.com/futuremojo/nlp-demystified/blob/main/notebooks/nlpdemystified_word_vectors.ipynb), we'll use the tokenizer's *texts_to_sequences* method to convert each sentence to a sequence of integers where each integer maps to a particular token.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences"
      ],
      "metadata": {
        "id": "OUE5rf_myuGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_seqs = sentence_tokenizer.texts_to_sequences(x_train)"
      ],
      "metadata": {
        "id": "Khb0e73Uy9F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_seqs[0])"
      ],
      "metadata": {
        "id": "3A5T7HDlPG04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the *sequences_to_texts* method to convert a vectorized sentence back to its preprocessed form.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#sequences_to_texts"
      ],
      "metadata": {
        "id": "77U2nCHKLC4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original: {x_train[0]}\")\n",
        "print(f\"Reconstructed: {sentence_tokenizer.sequences_to_texts([x_train_seqs[0]])}\")"
      ],
      "metadata": {
        "id": "Io3WhTXPPcBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll vectorize the labels (i.e. sequences of PoS tags) using its respective tokenizer."
      ],
      "metadata": {
        "id": "IrTae0CTLbzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_seqs = tag_tokenizer.texts_to_sequences(y_train)"
      ],
      "metadata": {
        "id": "px0r8Z8cPhqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_tokenizer.sequences_to_texts([y_train_seqs[0]])"
      ],
      "metadata": {
        "id": "Qzz8TCP6y6LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we'll do the same with the validation inputs and labels."
      ],
      "metadata": {
        "id": "tzMIDKL2Lov7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val_seqs = sentence_tokenizer.texts_to_sequences(x_val)\n",
        "y_val_seqs = tag_tokenizer.texts_to_sequences(y_val)"
      ],
      "metadata": {
        "id": "sZKg10BX5O-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we covered in the slides, **Recurrent Neural Networks** are capable of handling variable length sequences.<br><br>\n",
        "Despite that, it's still best to pad or truncate sequences to a uniform length for one or both of these reasons:<br>\n",
        "1. Performance. The longer a sequence, the higher the computation cost. One may want to truncate long sequences to a shorter length if that's feasible and doesn't result in too much performance loss.\n",
        "\n",
        "2. When processing datasets in batches, each sequence *in a batch* usually has to be of uniform length.<br>\n",
        "\n",
        "For simplicity, in this demo, we'll make *every* sequence be as long as the longest sequence. In other words, we'll determine how long the longest sequence is, then pad out the rest of the sequences to be the same length.<br>\n",
        "\n",
        "A more optimized solution would be to make each sequence as long as the longest sequence in each *batch* to avoid unnecessary processing."
      ],
      "metadata": {
        "id": "TfT-5dmCy_uJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = len(max(x_train_seqs, key=len))\n",
        "print(f\"Length of longest input sequence: {MAX_LENGTH}\")"
      ],
      "metadata": {
        "id": "rYLLph5iPng5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can pad the sequences with the *pad_sequences* method:<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"
      ],
      "metadata": {
        "id": "khQdjjfOQ5wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_padded = keras.preprocessing.sequence.pad_sequences(x_train_seqs, padding='post', \n",
        "                                                            maxlen=MAX_LENGTH)"
      ],
      "metadata": {
        "id": "B2F9536qPpbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train_padded[0])"
      ],
      "metadata": {
        "id": "-BKa6RNLPy1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll do the same with the training label (PoS sequences)..."
      ],
      "metadata": {
        "id": "XpbsTGrgRGNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_padded = keras.preprocessing.sequence.pad_sequences(y_train_seqs, padding='post', \n",
        "                                                            maxlen=MAX_LENGTH)"
      ],
      "metadata": {
        "id": "GDTo19V_P0CB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and the validation dataset."
      ],
      "metadata": {
        "id": "R4bOcVgLRK4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val_padded = keras.preprocessing.sequence.pad_sequences(x_val_seqs, padding='post', maxlen=MAX_LENGTH)\n",
        "y_val_padded = keras.preprocessing.sequence.pad_sequences(y_val_seqs, padding='post', maxlen=MAX_LENGTH)"
      ],
      "metadata": {
        "id": "OCtKDmLK5uq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PoS tagging is a multiclass classification task done at each timestep, so we need to convert every tag for every sentence into a one-hot encoding (we'll look at an alternative approach when we build a language model later in this demo).<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical<br>"
      ],
      "metadata": {
        "id": "UFQqWmck25sA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_categoricals = keras.utils.to_categorical(y_train_padded)"
      ],
      "metadata": {
        "id": "fZNu8P_nSv3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The label (PoS tag sequence) for a single sentence is now a **sequence of one-hot encodings**. These will serve as our training targets."
      ],
      "metadata": {
        "id": "X_Reegcq3iiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The one hot encodings for the first label.\n",
        "print(y_train_categoricals[0])"
      ],
      "metadata": {
        "id": "OPJlV1P_UStC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encoding for a single tag.\n",
        "print(y_train_categoricals[0][0])"
      ],
      "metadata": {
        "id": "0wzIkD9L3pz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can determine the PoS tag from a one-hot encoding by seeing which index is set to 1, then using that to query the tag tokenizer's *index_word* dictionary."
      ],
      "metadata": {
        "id": "TIxLTetGStWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.argmax(y_train_categoricals[0][0])\n",
        "print(f\"Index: {idx}\")\n",
        "\n",
        "print(f\"Tag: {tag_tokenizer.index_word[idx]}\")"
      ],
      "metadata": {
        "id": "OAqLUMuw319d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll one-hot encode the validation labels as well."
      ],
      "metadata": {
        "id": "CiZFnLm5TWj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_val_categoricals = keras.utils.to_categorical(y_val_padded)"
      ],
      "metadata": {
        "id": "lZMDn8E2357_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we're ready to build our model. We'll train word embeddings concurrently with our model (though you can use pretrained word vectors as well). If you're unfamiliar with this idea, refer to the [Word Vectors](https://www.nlpdemystified.org/course/word-vectors) module.<br><br>\n",
        "There are several new things here:<br>\n",
        "1. The embedding layer has a *mask_zero* parameter. We added padding in order to make our batches the same size, but we don't want the model to make PoS predictions on padding. Setting *mask_zero* to *True* makes the layers following the embedding layer ignore padding values.<br>\n",
        "https://www.tensorflow.org/guide/keras/masking_and_padding<br>\n",
        "https://stackoverflow.com/questions/47485216/how-does-mask-zero-in-keras-embedding-layer-work<br><br>\n",
        "2. We're using a **bidirectional LSTM**. The *Bidirectional* layer is a wrapper to which we pass an *LSTM* layer. The first parameter to the *LSTM* layer is the number of units in the cell. The second parameter, *return_sequences*, controls whether the RNN returns an output for each timestep or only the last output. Since we're doing PoS-tagging, we want an output for each timestep and so *return_sequences* is set to *True*.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\n"
      ],
      "metadata": {
        "id": "IdzikQR26NS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the embedding layer. \"+ 1\" to account for the padding token.\n",
        "num_tokens = len(sentence_tokenizer.word_index) + 1\n",
        "embedding_dim = 128\n",
        "\n",
        "# For the output layer. The number of classes corresponds to the \n",
        "# number of possible tags.\n",
        "num_classes = len(tag_tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "96PD9ncGQqO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The set_seed call and kernel_initializer parameters are used here to\n",
        "# ensure you and I get the same results. To get random weight initializations,\n",
        "# remove them.\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "model.add(layers.Embedding(input_dim=num_tokens, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=MAX_LENGTH,\n",
        "                           mask_zero=True))\n",
        "\n",
        "model.add(layers.Bidirectional(layers.LSTM(128, return_sequences=True, \n",
        "                                           kernel_initializer=tf.keras.initializers.random_normal(seed=1))))\n",
        "\n",
        "model.add(layers.Dense(num_classes, activation='softmax', \n",
        "                       kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "8enX4iWaQ-mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few notes about the model summary:<br>\n",
        "\n",
        "The embedding layer **output** has three dimensions:\n",
        "- Batch size (it's showing as \"None\" because we didn't specify it upfront. We'll do it when we call *model.fit*).\n",
        "- Sequence length (the sequences are all the same length now after our padding step).\n",
        "- Embedding dimension.\n",
        "<br><br>\n",
        "\n",
        "The LSTM outputs a vector *twice* the size of what we specified because it's bidirectional. Recall from the slides that the outputs from the two LSTMs will be concatenated before going to the output layer.\n",
        "<br><br>\n",
        "\n",
        "The final layer's **output** also has three dimensions:\n",
        "- Batch size\n",
        "- Sequence length\n",
        "- Output dimension (the number of possible tags).\n",
        "\n",
        "The output will be a **sequence of probability distributions** for each input sequence. One probability distribution per tag.\n",
        "\n"
      ],
      "metadata": {
        "id": "s-RPX0V86btV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "k2mAahG9U7lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we did in previous demos, we'll use *early stopping* with some *patience* to halt training once validation loss stops improving.<br>\n",
        "\n",
        "The model will compare its output (sequences of softmax-generated probability distributions) against the one-hot encoded targets.\n",
        "\n"
      ],
      "metadata": {
        "id": "l0XDusUT-Uew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "history = model.fit(x_train_padded, y_train_categoricals, epochs=20, \n",
        "                    batch_size=256, validation_data=(x_val_padded, y_val_categoricals), \n",
        "                    callbacks=[es_callback])"
      ],
      "metadata": {
        "id": "V0HuoNKrU8si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once our model is trained, we'll vectorize and pad the testing dataset. In the case of the labels, we'll also one-hot encode them."
      ],
      "metadata": {
        "id": "WSRTSU_PFF_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the test data and test the model.\n",
        "x_test_seqs = sentence_tokenizer.texts_to_sequences(x_test)\n",
        "x_test_padded = keras.preprocessing.sequence.pad_sequences(x_test_seqs, padding='post', maxlen=MAX_LENGTH)\n",
        "\n",
        "y_test_seqs = tag_tokenizer.texts_to_sequences(y_test)\n",
        "y_test_padded = keras.preprocessing.sequence.pad_sequences(y_test_seqs, padding='post', maxlen=MAX_LENGTH)\n",
        "y_test_categoricals = keras.utils.to_categorical(y_test_padded)"
      ],
      "metadata": {
        "id": "NjBNMiGyaoRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test_padded, y_test_categoricals)"
      ],
      "metadata": {
        "id": "wTJ3HvUxVGVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use our model to tag sentences."
      ],
      "metadata": {
        "id": "5EnxAd_UbWpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [\n",
        "    \"Brown refused to testify.\",\n",
        "    \"Brown sofas are on sale.\",\n",
        "]"
      ],
      "metadata": {
        "id": "vpdBrMTKYC2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function below takes a list of strings, tokenizes and pads them, then has the model tag them. Note that if a sentence is longer than MAX_LENGTH, it'll be truncated."
      ],
      "metadata": {
        "id": "9Pk-IzuDN7Fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_sentences(sentences):\n",
        "  sentences_seqs = sentence_tokenizer.texts_to_sequences(sentences)\n",
        "  sentences_padded = keras.preprocessing.sequence.pad_sequences(sentences_seqs, \n",
        "                                                                maxlen=MAX_LENGTH, \n",
        "                                                                padding='post')\n",
        "\n",
        "  # The model returns a LIST of PROBABILITY DISTRIBUTIONS (due to the softmax)\n",
        "  # for EACH sentence. There is one probability distribution for each PoS tag.\n",
        "  tag_preds = model.predict(sentences_padded)\n",
        "\n",
        "  sentence_tags = []\n",
        "\n",
        "  # For EACH LIST of probability distributions...\n",
        "  for i, preds in enumerate(tag_preds):\n",
        "\n",
        "    # Extract the most probable tag from EACH probability distribution.\n",
        "    # Note how we're extracting tags for only the non-padding tokens.\n",
        "    tags_seq = [np.argmax(p) for p in preds[:len(sentences_seqs[i])]]\n",
        "\n",
        "    # Convert the sentence and tag sequences back to their token counterparts.\n",
        "    words = [sentence_tokenizer.index_word[w] for w in sentences_seqs[i]]\n",
        "    tags = [tag_tokenizer.index_word[t] for t in tags_seq]\n",
        "    sentence_tags.append(list(zip(words, tags)))\n",
        "\n",
        "  return sentence_tags\n"
      ],
      "metadata": {
        "id": "eJhZy6Qtbfgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagged_sample_sentences = tag_sentences(samples)"
      ],
      "metadata": {
        "id": "epf-F8q79U8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tagged_sample_sentences[0])"
      ],
      "metadata": {
        "id": "1n8DlpC3fx78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tagged_sample_sentences[1])"
      ],
      "metadata": {
        "id": "Zu9RsRzPe1Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So that's one way to build a PoS tagger. Industrial-strength taggers use a lot more data and these days, are powered by more sophisticated models which we'll learn about when we cover *transformers*."
      ],
      "metadata": {
        "id": "2gTzEc6Yd_Bt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Language Modelling With Stacked LSTMs"
      ],
      "metadata": {
        "id": "aN6_SU-WPaMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll build a language model trained on the *Art of War* by Sun Tzu."
      ],
      "metadata": {
        "id": "Ay1F6VQZPeKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "art_of_war = requests.get('https://raw.githubusercontent.com/futuremojo/nlp-demystified/main/datasets/art_of_war.txt')\\\n",
        "                     .text\n",
        "\n",
        "art_of_war[:300]"
      ],
      "metadata": {
        "id": "ZEG0mxX0hk7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The language model we'll build will be **character**-based (as opposed to word-based). That is, given a sequence of one or more characters, the model will be asked to predict the next character.<br><br>\n",
        "Character-level models have the advantage of:\n",
        "- Smaller prediction space. There are only a handful of characters in the English language compared to the tens of thousands of words in a typical corpus.\n",
        "- Character-level models are more resilient to out-of-vocabulary (OOV) conditions and are better able to learn the lower mechanics of language (including punctuation).<br><br>\n",
        "\n",
        "On the other hand, character-level models need to learn a sequence of characters to \"make sense\" of a word (e.g. the sequence of \"c\", \"a\", \"t\" to identify \"cat\" as a pattern) which can be inefficient and result in lower performance.<br><br>\n",
        "RNNs can process any kind of sequence so what's shown here can easily be applied at the word level. When we cover *transformers*, we'll learn an alternative tokenization technique called **subword tokenization** that is a middle-ground between these two approaches."
      ],
      "metadata": {
        "id": "BkHpwtcGUvaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll initialize a Keras **Tokenizer** and set the *char_level* parameter to True so that our corpus gets tokenized into characters rather than words. It'll continue lower-casing and applying its default light filtering.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer"
      ],
      "metadata": {
        "id": "XNkrCoktissC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)"
      ],
      "metadata": {
        "id": "mWdTaZyDNe7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts([art_of_war])"
      ],
      "metadata": {
        "id": "5K3Z3xiYNe-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer's internal dictionary now maps characters rather than words..."
      ],
      "metadata": {
        "id": "SYSjvX_-jxcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_config()"
      ],
      "metadata": {
        "id": "DmrRyC8eNfCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and the resulting possibility space is much smaller."
      ],
      "metadata": {
        "id": "gk0m9YXhjw4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Tokenizer \\\"Vocabulary\\\" size: {len(tokenizer.word_index)}\")"
      ],
      "metadata": {
        "id": "xYVGJN0ENfE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we did when building the PoS tagger, we'll vectorize the book's characters into a sequence of integers, each integer mapping to a particular character."
      ],
      "metadata": {
        "id": "nVihxn9hj8ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seq = tokenizer.texts_to_sequences([art_of_war])[0]"
      ],
      "metadata": {
        "id": "Z-ZpUDbnNfHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Text length: {len(seq)}\")"
      ],
      "metadata": {
        "id": "31omC4dINfKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check.\n",
        "tokenizer.sequences_to_texts([seq[:10]])"
      ],
      "metadata": {
        "id": "6dSf6yi8yDSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our training data is currently one long sequence which we'll need to segment into training examples. To do this, we'll use the **Tensorflow Data** API which makes it easy to build preprocessing pipelines by chaining operations together.<br>\n",
        "https://www.tensorflow.org/guide/data<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/data<br>"
      ],
      "metadata": {
        "id": "tnEjEAzBkR9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use this API, we need to first convert our vectorized corpus (which is a plain Python list) into a  **Dataset** object, which we can do using the *from_tensor_slices* method. This takes our vectorized corpus and returns a stream of tensors, one tensor for each integer. We'll then be able to perform operations on this **Dataset** object to prep our data.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset"
      ],
      "metadata": {
        "id": "1V6igK_9wJjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slices = tf.data.Dataset.from_tensor_slices(seq)\n",
        "type(slices)"
      ],
      "metadata": {
        "id": "HA6Av1ZC58KW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like Python **generators**, TF **Datasets** function like iterators, so we need to do things like convert them into a list or iterate over them in order to print them.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take"
      ],
      "metadata": {
        "id": "8If8jVoXxReD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Dataset from the first ten slices and convert to a list to output to console.\n",
        "list(slices.take(10))"
      ],
      "metadata": {
        "id": "rKO3vdcWyNm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The first ten elements from the original vectorized corpus.\n",
        "seq[:10]"
      ],
      "metadata": {
        "id": "O8XQ464jyYCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to create the training examples. To do that, we'll use the *window* method. Calling *window* on a dataset results in a sequence of datasets each containing N elements. So calling window(100) on a dataset of 1000 will result in a sequence of 10 datasets, each containing 100 elements.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#window"
      ],
      "metadata": {
        "id": "X1c-ihOByy88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we're creating windows of `input_timesteps + 1`. The *input_timesteps* represents our training example length. The *+1* is there to help us create the target/label for each training example. This will be clarified further below.<br><br>\n",
        "In addition, we're setting *shift* to 1. This means we'll get overlapping windows shifted by 1. e.g. if the input is [1, 2, 3, 4, ...]. The first window will contain [1, 2, 3, ...], the second window will contain [2, 3, 4, ...] and so on. This is so we can have more training examples.<br><br>\n",
        "Finally, we're setting *drop_remainder* to True which ensures ALL windows contain exactly N elements. i.e. once the input contains fewer than N elements, they are ignored."
      ],
      "metadata": {
        "id": "YPGcuQTQz7Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_timesteps = 100\n",
        "window_size = input_timesteps + 1\n",
        "windows = slices.window(window_size, shift=1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "ktpU39wkvHJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the first few windows, we can see they're all the same length and that each subsequent window is shifted over by 1. Our corpus has now been divided into segments of length `input_timesteps + 1`."
      ],
      "metadata": {
        "id": "_ZPxcG8W1_1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for w in windows.take(3):\n",
        "  arr = list(w.as_numpy_iterator())\n",
        "  print(len(arr), arr)"
      ],
      "metadata": {
        "id": "m5yh94YQvHNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *window* method returns a nested dataset of datasets (i.e. each window is a dataset containing a tensor)."
      ],
      "metadata": {
        "id": "0tlBHrkh3a_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(windows, '\\n')\n",
        "\n",
        "for w in windows.take(2):\n",
        "  print(w)"
      ],
      "metadata": {
        "id": "aUlSKqMIv4DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But our model won't accept those. It'll accept only tensors, so we need to extract the tensors from each window. To do that, we'll use *flat_map* which will flatten the dataset of datasets into a single dataset of elements. But because we want to retain our segmented sequences, we'll also pass in a *batch* function to maintain the segments (otherwise, we'll just get back one large tensor representing our whole corpus).<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#flat_map<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch"
      ],
      "metadata": {
        "id": "1FeagND73tID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = windows.flat_map(lambda window: window.batch(window_size))"
      ],
      "metadata": {
        "id": "caKhWf45vHQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have a single dataset of tensors, where each tensor is `input_timesteps+1` long and shifted by 1."
      ],
      "metadata": {
        "id": "-VEWbChD3xRT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for d in dataset.take(2):\n",
        "  print(d)"
      ],
      "metadata": {
        "id": "86Kj0agg3UaF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to create batches from our dataset. To do this, we'll shuffle the dataset, then create batches.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle<br>\n"
      ],
      "metadata": {
        "id": "QLY8M5jA47VX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32"
      ],
      "metadata": {
        "id": "2GgpocDSvHYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The expected number of batches should be (len(seq) - input_timesteps) / batch_size\n",
        "batches = dataset.shuffle(10000).batch(batch_size)"
      ],
      "metadata": {
        "id": "Wlg3yFIJ8OmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for b in batches.take(2):\n",
        "  print(b)"
      ],
      "metadata": {
        "id": "qsa1nIFqyDpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now separate each example into an input sequence(x) and a corresponding label/target sequence(y).<br><br>\n",
        "In the slides, we talked about **Teacher Forcing** where:<br>\n",
        "1. At each timestep during training, the output is compared to a label.\n",
        "2. At the next timestep, rather than feeding the model the previous output, we feed it the next character of the input sequence (i.e. what the model should've outputted).\n",
        "<br><br>\n",
        "\n",
        "This is why each sequence is of size *input_timesteps + 1*. Each sequence is now going to be separated into TWO sequences. The first sequence will be the training input and will be of length *input_timesteps* (i.e. everything but the LAST character). The second sequence will be the label/target and will consist of all the sequence elements shifted by 1 (i.e. everything but the FIRST character). If this is confusing, refer to the accompanying video for this module where we cover training language models.<br><br>\n",
        "\n",
        "So if a sequence is \"she swam in the lake\", then:\n",
        "- The input will be \"she swam in the lak\" (drop the last character)\n",
        "- The target/label will be \"he swam in the lake\" (drop the first character)"
      ],
      "metadata": {
        "id": "feeAcKdY7fU1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xy_batches = batches.map(lambda batch: (batch[:, :-1], batch[:, 1:]))"
      ],
      "metadata": {
        "id": "opFC6gMn7p_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each batch now consists of a set of input sequences and a corresponding set of label/target sequences, with the labels/targets shifted by 1."
      ],
      "metadata": {
        "id": "QV0FZN41-ASJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for b in xy_batches.take(1):\n",
        "  print(b)"
      ],
      "metadata": {
        "id": "A7iErZRc7sTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For greater clarity, this is the first input sequence from the first batch,\n",
        "# and it's corresponding label/target sequence.\n",
        "for b in xy_batches.take(1):\n",
        "  print(\"x1 length: \", len(b[0][0].numpy()))\n",
        "  print(\"x1: \", b[0][0].numpy())\n",
        "  print(\"\\n\")\n",
        "  print(\"y1 length: \", len(b[1][0].numpy()))\n",
        "  print(\"y1: \", b[1][0].numpy())"
      ],
      "metadata": {
        "id": "WL0sd8B0y4gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last step before we can build our model is to one-hot encode the **inputs**. We're doing this because:\n",
        "1. We're not using embeddings for the input. We can, but since this is a character model with just a few dozen possible choices, we can get away with one-hot encoding. There's also no reason to think a particular letter should be closer to another in vector space as we would want in a word-level model.\n",
        "\n",
        "2. Since we're not using embeddings and our input is categorical, we need to one-hot encode.\n",
        "\n",
        "Note that despite our labels ALSO being categorical, we are NOT one-hot encoding them this time. This is because we'll be using a loss function that can help us skip that step (more below)."
      ],
      "metadata": {
        "id": "2DgNpgicAMbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_tokens = len(tokenizer.word_index) + 1\n",
        "\n",
        "# One-hot encode the input sequences, don't do anything with the label/target sequences.\n",
        "xy_batches = xy_batches.map(lambda inputs, labels: (tf.one_hot(inputs, depth=num_tokens), labels))"
      ],
      "metadata": {
        "id": "hxEGMY5Cy4mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each input sequence is now a sequence of one-hot encodings.\n",
        "for b in xy_batches.take(1):\n",
        "  print(\"x1: \", b[0][0].numpy())\n",
        "  print(\"\\n\")\n",
        "  print(\"y1: \", b[1][0].numpy())"
      ],
      "metadata": {
        "id": "0ENgyGB2y4o_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we've:\n",
        "- Segmented our corpus into fixed-length sequences.\n",
        "- Created training and label/target sequences.\n",
        "- Organized them into batches.\n",
        "\n",
        "The last step is to add some **prefetching**. This is an optimization step. This way, while the model trains on the current batch of data, the pipeline reads and prepares the next batch.<br>\n",
        "https://www.tensorflow.org/guide/data_performance#prefetching"
      ],
      "metadata": {
        "id": "H4yAf-VfG7nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "2G6p9SOky4rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now build our model. There are three new things here:\n",
        "1. We're stacking two LSTMs. The sequential output of the first LSTM will become the sequential input to the second LSTM.<br><br>\n",
        "2. We're adding some *recurrent_dropout*. This drops connections between the recurrent units (i.e. the dropout is applied horizontally across time). You can still use regular *dropout* as well which will be applied to the inputs/outputs. Refer to this paper for more information:<br>\n",
        "https://arxiv.org/abs/1512.05287<br><br>\n",
        "3. We're using **sparse_categorical_crossentropy**. This allows us to provide labels as integers for multiclass classification rather than one-hot encodings.<br>\n",
        "https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class"
      ],
      "metadata": {
        "id": "u01wPGyqK5wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "model.add(layers.LSTM(128, return_sequences=True, input_shape=[None, num_tokens], recurrent_dropout=0.2))\n",
        "model.add(layers.LSTM(128, return_sequences=True, input_shape=[None, num_tokens], recurrent_dropout=0.2))\n",
        "\n",
        "model.add(layers.Dense(num_tokens, activation='softmax'))\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam')\n"
      ],
      "metadata": {
        "id": "Ka3xgXNDy4wq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "ELfe3-O_y4zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because this model takes a few hours to train, we're using **model checkpoints** to save the weights after every epoch. This way, if something goes wrong with our system during training, we can reload the last set of weights from the checkpoint, and resume training from there.<br>\n",
        "https://keras.io/api/callbacks/model_checkpoint/"
      ],
      "metadata": {
        "id": "8SS9vRezNbCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filepath=\"./ArtofWarLM/training1/cp.ckpt\"\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=filepath,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "metadata": {
        "id": "VBXSpEIQy437"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When calling the model's *fit* method, we:<br>\n",
        "1. simply pass in the batches as is (no need to separate into explicit x and y arguments).\n",
        "2. pass the model checkpoint callback to save the weights after every epoch\n",
        "\n",
        "**NOTE**: The call to *fit* below is commented out. Because this model takes a few hours to train, I trained it ahead of time and saved it. If you want to train it yourself, feel free to uncomment and execute it. Also, because of the random weight initialization, your trained model's output will likely differ from mine."
      ],
      "metadata": {
        "id": "bExv-bELNxMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# history = model.fit(xy_batches, epochs=50, callbacks=[cp_callback])\n",
        "#"
      ],
      "metadata": {
        "id": "152rPbsMy46W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once training is complete, we can call the model's *save* method to save its weights and metadata. Again, this is commented out because I already trained the model previously.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/Model#save<br>\n",
        "https://www.tensorflow.org/guide/keras/save_and_serialize"
      ],
      "metadata": {
        "id": "5K2uM9ZuOdU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# model.save('art_of_war_char_level_lm')\n",
        "#"
      ],
      "metadata": {
        "id": "rncmIneRy487"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and unzip the previously trained model..."
      ],
      "metadata": {
        "id": "H-4LQRthQ-XN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/futuremojo/nlp-demystified/raw/main/models/art_of_war_char_level_lm.zip\n",
        "!unzip -o art_of_war_char_level_lm.zip"
      ],
      "metadata": {
        "id": "fA2om-61cm-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and load it.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model"
      ],
      "metadata": {
        "id": "BFUVEoC4hVIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.load_model('art_of_war_char_level_lm')"
      ],
      "metadata": {
        "id": "q-5khVOuy4_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a trained model, let's generate some text.<br><br>\n",
        "The function below takes some seed text and uses that to generate a certain number of characters. For each character, it uses the generated text so far as the input. It's not the most efficient function but it'll work here.<br><br>\n",
        "There's also a *temperature* parameter. The next character is picked from a probability distribution. By dividing the log of this distribution by *temperature*, we can influence the randomness of the output.<br><br>\n",
        "When the temperature is low (< 1), the probability distribution sharpens and the model will be more strict in recreating the original text. As we raise the temperature, the distribution flattens and there's a higher chance the model picks something unexpected, resulting in greater surprise in the output. In practice, a high enough temperature will result in nonsense."
      ],
      "metadata": {
        "id": "6xaiAYc1RLrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, seed_text, num_chars=200, temperature=1):\n",
        "\n",
        "  text = seed_text  \n",
        "\n",
        "  for _ in range(num_chars):\n",
        "\n",
        "    # Take the last *input_timesteps* number of characters in the text so far\n",
        "    # as input.\n",
        "    input = np.array(tokenizer.texts_to_sequences([text[-input_timesteps:]]))\n",
        "    input = tf.one_hot(input, num_tokens)\n",
        "\n",
        "    # Create probability distribution for next character adjusted by temperature.\n",
        "    preds = model.predict(input)[0, -1:, :] # <-- We want only the last character so we're extracting the softmax output for that.\n",
        "    preds = tf.math.log(preds) / temperature\n",
        "\n",
        "    # Sample next character and add to running text.\n",
        "    next_char = tf.random.categorical(preds, num_samples=1)\n",
        "    next_char = tokenizer.sequences_to_texts(next_char.numpy())[0]\n",
        "\n",
        "    text += next_char\n",
        "\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "jiwXXCn7JKea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "print(generate_text(model, tokenizer, \"Banana peels on the battlefield can\", num_chars=300, temperature=0.2))"
      ],
      "metadata": {
        "id": "hywFzqgFGb43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, tokenizer, \"It's time to release the Kraken when\", num_chars=300, temperature=0.5))"
      ],
      "metadata": {
        "id": "GMVFH66EMfXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, tokenizer, \"Crush your enemies, see them driven before you, and\", num_chars=300, \n",
        "                    temperature=1))"
      ],
      "metadata": {
        "id": "0LAiIE6IGb1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, tokenizer, \"What is best in life?\", num_chars=300, temperature=2))"
      ],
      "metadata": {
        "id": "oEALULBOZ75z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few observations of the preceding outputs:\n",
        "1. Despite being a character-level model, the model managed to \"learn\" spelling, cadence, punctuation, spacing, grammar, and even numbered bullet points just from trying to predict the next character.\n",
        "\n",
        "2. It's pretty cool how the model manages to take our initial seed text and complete a sentence with it before moving on.\n",
        "\n",
        "3. We can see the output getting increasingly nonsensical as the temperature rises. What temperature to use ultimately depends on the nature of your corpus and your goals with the language model.\n",
        "\n",
        "Also, in contrast to our language model, GPT-3 has 175 billion parameters and was trained on 45 terabytes of data, but the high-level principle of learning through prediction remains the same."
      ],
      "metadata": {
        "id": "SuqUZcGkUAvd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further Exploration\n",
        "1. Check out *Sunspring*, a sci-fi short written by an LSTM. The director and actors played the script straight and the result is hilarious.<br>\n",
        "https://www.youtube.com/watch?v=LY7x2Ihqjmc<br>\n",
        "https://en.wikipedia.org/wiki/Sunspring<br><br>\n",
        "2. Everything we learned here can be applied at the word-level. Try creating a word-level language model with a different corpus (maybe download something from https://www.gutenberg.org/) and try using word embeddings.<br><br>\n",
        "3. We didn't evaluate our language model using perplexity. Find out online how to do it."
      ],
      "metadata": {
        "id": "jY9ICurtUoDs"
      }
    }
  ]
}
