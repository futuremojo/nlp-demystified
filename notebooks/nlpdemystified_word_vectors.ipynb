{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlpdemystified-word-vectors.ipynb",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/futuremojo/nlp-demystified/blob/main/notebooks/nlpdemystified_word_vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing Demystified | Word Vectors\n",
        "https://nlpdemystified.org<br>\n",
        "https://github.com/futuremojo/nlp-demystified<br><br>\n",
        "Course module for this demo: https://www.nlpdemystified.org/course/word-vectors"
      ],
      "metadata": {
        "id": "UuPdnyWXh7vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**<br>\n",
        "Enable **GPU acceleration** by going to *Runtime > Change Runtime Type*. Keep in mind that, on certain tiers, you're not guaranteed GPU access depending on usage history and current load.\n",
        "<br><br>\n",
        "Also, if you're running this in the cloud rather than a local Jupyter server on your machine, then the notebook will *timeout* after a period of inactivity.\n",
        "<br><br>\n",
        "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
        "https://research.google.com/colaboratory/local-runtimes.html"
      ],
      "metadata": {
        "id": "91LZr9ogtHi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To demonstrate word vectors, we're going to use **Gensim** which we first encountered in the [topic modelling](https://www.nlpdemystified.org/course/topic-modelling) module.<br>\n",
        "\n",
        "At the time of this recording, the default Gensim version in Colab was 3.X, so we'll first upgrade to 4.X."
      ],
      "metadata": {
        "id": "qOE58pxDZqah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upgrade gensim just in case.\n",
        "!pip install -U gensim==4.*"
      ],
      "metadata": {
        "id": "2vNBSTzIKkRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "import tensorflow as tf\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "kIMAMpBaoqkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**<br>\n",
        "In this notebook, we won't train standalone word embeddings from scratch. Rather, we'll:\n",
        "1. Use *pretrained* embeddings in one model.\n",
        "2. Train embeddings alongside another model.\n",
        "<br>\n",
        "\n",
        "If you want to try training standalone word embeddings, coding Skip-Gram With Negative Sampling (SGNS) from scratch shouldn't be too hard now that you know all the details. But I recommend just using the **Gensim** library instead:<br>\n",
        "https://radimrehurek.com/gensim/models/word2vec.html<br>\n",
        "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
        "\n"
      ],
      "metadata": {
        "id": "SBQlbyiVY3WJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Pretrained, Third-Party Vectors"
      ],
      "metadata": {
        "id": "A_rgKP1vJ2kT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a variety of pretrained, static word vector packages out there. In this section, we'll use the **Google News** vectors, a collection of three million, 300-dimension word vectors trained from three billion words from a Google News corpus (circa 2015)."
      ],
      "metadata": {
        "id": "RHMDs5FPKXUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need to first download the actual word vectors. It's over a gigabyte in size but will fit within our environment."
      ],
      "metadata": {
        "id": "lQEAXBoBMidI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"https://drive.google.com/uc?id=17Vf3ucDBnHMarELN_yYaRu0lHXdulGTZ\""
      ],
      "metadata": {
        "id": "dIKSA-KMbj8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_file = './GoogleNews-vectors-negative300.bin.gz'"
      ],
      "metadata": {
        "id": "6FtVSlPQbj4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll have **gensim** load the vectors through the **KeyedVectors** module which will enable us to look up vectors by tokens and indices.<br>\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html\n",
        "<br><br>\n",
        "To save time and space, we'll limit ourselves to 200,000 word vectors for now."
      ],
      "metadata": {
        "id": "BXQRngPaPWhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True, limit=200000)"
      ],
      "metadata": {
        "id": "dmRUiKPObjqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieving a word's vector is a matter of using a token as a key."
      ],
      "metadata": {
        "id": "ir22T0t3QqPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pizza = word_vectors['pizza']\n",
        "print(f'Vector dimension: {pizza.shape}')\n",
        "\n",
        "# The embedding for the word 'pizza'.\n",
        "print(pizza)"
      ],
      "metadata": {
        "id": "wm9lDs3Vfv12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get the cosine similarity between two words using the *similarity* method.\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.similarity"
      ],
      "metadata": {
        "id": "ORLN9U9fq8hp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we see words which we expect to be related have a higher similarity measure..."
      ],
      "metadata": {
        "id": "G-7Iuo2orp60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_vectors.similarity('pizza', 'tomato'))\n",
        "print(word_vectors.similarity('pizza', 'sauce'))\n",
        "print(word_vectors.similarity('pizza', 'cheese'))"
      ],
      "metadata": {
        "id": "r1i6H88Cfp1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...compared to words we don't expect to be related."
      ],
      "metadata": {
        "id": "6UWdQJzaryQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_vectors.similarity('pizza', 'gorilla'))\n",
        "print(word_vectors.similarity('pizza', 'tree'))\n",
        "print(word_vectors.similarity('pizza', 'yoga'))"
      ],
      "metadata": {
        "id": "YUXR8uQ0ftWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out-of-vocabulary (OOV) words thrown an exception. Bear in mind that looking up even common words here can result in an exception because we loaded only a subset of the vocabulary."
      ],
      "metadata": {
        "id": "g3cD4P1tr4fT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  word_vectors['womblyboo']\n",
        "except KeyError as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "zLf9bpYihVz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compare two sentences by using *n_similarity* which computes the cosine similarity of two **sets** of words.\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.n_similarity<br><br>\n",
        "*n_similarity* expects a sentence as a list of words, hence the use of '.split()'. It'll take this list of words, calculate the average of its word vectors, and use the result as an embedding for the whole sentence."
      ],
      "metadata": {
        "id": "XCQcU72-q0V5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.n_similarity(\"dog bites man\".split(), \"canine nips human\".split())"
      ],
      "metadata": {
        "id": "nwvDT25miXk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.n_similarity(\"martian dolphins are hostile\".split(), \"i flunked calligraphy school\".split())"
      ],
      "metadata": {
        "id": "_rgATfqBisrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One downside of this approach is that word order is thrown out, so two sentences with identical words which mean different things would score a perfect similarity score."
      ],
      "metadata": {
        "id": "BZLcvAU4vf6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.n_similarity(\"dog bites man\".split(), \"man bites dog\".split())"
      ],
      "metadata": {
        "id": "yikNOChTinp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing quick-and-dirty similarity measures like this is probably best if your corpus is domain-specific and similarity is based more on keywords. The more specific, the better.<br><br>\n",
        "For example, a corpus of business news headlines would probably work well."
      ],
      "metadata": {
        "id": "DLLpgr06vxTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = \"Volkswagen intends to double electric car sales in China\".lower().split()\n",
        "s2 = \"First Toyota with solid state battery will be hybrid\".lower().split()\n",
        "word_vectors.n_similarity(s1, s2)"
      ],
      "metadata": {
        "id": "EejHDg24iyum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *most_similar* method returns the words with the closest vectors.<br>\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar"
      ],
      "metadata": {
        "id": "aWsFWJUXQ9RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.most_similar(positive=['cell'], topn=10)"
      ],
      "metadata": {
        "id": "2f2CXSQNxZgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also combine vectors first, and retrieve the words most similar to their mean. Here, we're combining the vectors for 'cell' and 'phone' and retrieving the vectors closest to that result."
      ],
      "metadata": {
        "id": "1YN45ETCReXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.most_similar(positive=['cell', 'phone'], topn=10)"
      ],
      "metadata": {
        "id": "wvOkR7UWx1su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a collection of words, the *doesn't_match* method returns the word that \"doesn't go\" with the rest (i.e. with the vector that's furthest away from the mean of all the other vectors).<br>\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.doesnt_match"
      ],
      "metadata": {
        "id": "PjMdhqJvR7ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.doesnt_match([\"apple\", \"orange\", \"hamburger\", \"banana\", \"kiwi\"])"
      ],
      "metadata": {
        "id": "oA2NVKHPzExZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the power of context in this example with 'Toyota' being correctly identified as the odd one out."
      ],
      "metadata": {
        "id": "Xajz9UFQR91b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.doesnt_match([\"Microsoft\", \"Apple\", \"Toyota\", \"Amazon\", \"Netflix\", \"Google\"])"
      ],
      "metadata": {
        "id": "evIAtmcVygyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing word vectors is straight-forward and can offer insights into what kind of contexts the training algorithm picked up.<br><br>\n",
        "Because these word vectors have a dimension of 300, we need to reduce them down to two dimensions to plot them on a regular graph. This can be done through **Principal Components Analysis (PCA)**:<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html<br>\n",
        "<br>\n",
        "Here, we're plotting the words we considered in the slides."
      ],
      "metadata": {
        "id": "h6UyREbVUS6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_pca_scatterplot(model, words):\n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r', s=128)\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)"
      ],
      "metadata": {
        "id": "wC1e7EF0-b-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_pca_scatterplot(word_vectors, ['swim', 'swimming', 'cat', 'dog', 'feline', 'road', 'car', 'bus'])"
      ],
      "metadata": {
        "id": "cqmNesyaEhq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can even solve analogies (to a limited extent) with vector arithmetic.<br><br>\n",
        "Here, we're solving the analogy:<br>\n",
        "_Rome is to Italy as London is to __________.<br><br>\n",
        "Arithmetically, this is Italy + London - Rome.\n"
      ],
      "metadata": {
        "id": "EXE5AEEUSnEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.most_similar(positive=['Italy', 'London'], negative=['Rome'], topn=3)"
      ],
      "metadata": {
        "id": "VCJZYi-1zNUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing it can help with geometric intuition."
      ],
      "metadata": {
        "id": "vunBH2R5XRWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_pca_scatterplot(word_vectors, ['Rome', 'Italy', 'London', 'Britain', 'UK'])"
      ],
      "metadata": {
        "id": "RlH9W8q2V0Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Pretrained Word Vectors for Classification"
      ],
      "metadata": {
        "id": "iuDWv6NXXjoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we'll train a **Keras** model to use these Google News vectors to perform sentiment analysis on a bunch of **Yelp** reviews.\n",
        "<br><br>\n",
        "For this model, we'll increase the number of word vectors loaded to 1,000,000.\n",
        "\n"
      ],
      "metadata": {
        "id": "Am-9bvppYKuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True, limit=1000000)"
      ],
      "metadata": {
        "id": "VVLj4fSMzvHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset we'll use is *Yelp Polarity Reviews*, a collection of ~600,000 reviews for both training and testing.<br><br>\n",
        "The original Yelp reviews use a five-star rating system. The ratings in this dataset have been modified to simply be negative (label==1) or positive (label==2).<br>\n",
        "https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews<br><br>\n",
        "Tensorflow comes with a datasets loader but we're going to download the file manually and process the data ourselves for completeness."
      ],
      "metadata": {
        "id": "vdZQNavVYxfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz\""
      ],
      "metadata": {
        "id": "RcMEE11bzz_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzipping the archive results in *train.csv* and *test.csv* files placed in the default *contents* folder of our environment."
      ],
      "metadata": {
        "id": "wnTnApF4aq7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xvzf /root/input/yelp_review_polarity_csv.tgz\n",
        "\n",
        "# Show current working directory.\n",
        "!pwd"
      ],
      "metadata": {
        "id": "-UK1y9kjzz7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Pandas** library makes it simple to load a CSV file into memory and manipulate the data.<br>\n",
        "https://pandas.pydata.org/<br>\n",
        "https://pandas.pydata.org/docs/<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html?highlight=read_csv"
      ],
      "metadata": {
        "id": "OaJCSM3lbK-E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we're loading the CSV into a Pandas **dataframe** (sort of like an in-memory table) and explicitly naming the columns.<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html?highlight=dataframe#pandas.DataFrame"
      ],
      "metadata": {
        "id": "xIG4UVMkbq9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_train = pd.read_csv('yelp_review_polarity_csv/train.csv', names=['sentiment', 'review'])\n",
        "print(yelp_train.shape)"
      ],
      "metadata": {
        "id": "tdLoK1Jozz0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get a quick view of the data through the *head* method.<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html"
      ],
      "metadata": {
        "id": "lGgJl1QCb738"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_train.head()"
      ],
      "metadata": {
        "id": "jVHtVqOBzzwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save on training time, we'll train on 100,000 reviews rather than the full set. To do that, we'll shuffle the dataset using the *sample* method and *copy* the first 100,000 entries. The reason to shuffle first is to ensure we get a mix of reviews from a variety of businesses (in case the data is sorted in some way).<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html\n"
      ],
      "metadata": {
        "id": "9wS0o4gEclCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = 100000\n",
        "yelp_train = yelp_train.sample(frac=1, random_state=1)[:TRAIN_SIZE].copy()\n",
        "print(yelp_train.shape)"
      ],
      "metadata": {
        "id": "YacSH4F0c5aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next thing to do is adjust the labels. This is a **binary classification problem**, so our model's output layer will be a single unit with a **sigmoid** activation function. This function's output will be between 0 and 1 which is then compared against the training label. But the labels are currently 1 for negative, and 2 for positive, which is going to cause problems when calculating the loss.<br><br>\n",
        "So we'll simply replace the 1s with 0s, and 2s with 1s using the *replace* method.<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
        "<br><br>\n",
        "Alternatively, we could keep the labels as-is and treat this as a **multiclassification** problem with two labels and use a **softmax**, but we would then need to **one-hot encode** the labels (at least based on what we've learnt so far).\n"
      ],
      "metadata": {
        "id": "DLraxnTMd9iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_train['sentiment'].replace(to_replace=1, value=0, inplace=True)\n",
        "yelp_train['sentiment'].replace(to_replace=2, value=1, inplace=True)"
      ],
      "metadata": {
        "id": "kbmYKm-SjGnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_train.head()"
      ],
      "metadata": {
        "id": "LlzWLECijrg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we've done throughout this course, we'll create train/validation splits.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ],
      "metadata": {
        "id": "rILnYdQMgIlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_train_split, yelp_val_split = train_test_split(yelp_train, train_size=0.85, random_state=1)"
      ],
      "metadata": {
        "id": "CC-VSNcn7Cpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training data.\n",
        "train_reviews = yelp_train_split['review']\n",
        "y_train = np.array(yelp_train_split['sentiment'])\n",
        "\n",
        "# Set up validation data.\n",
        "val_reviews = yelp_val_split['review']\n",
        "y_val = np.array(yelp_val_split['sentiment'])"
      ],
      "metadata": {
        "id": "9Xj4WyLb7Wdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick sanity check to see how our data is distributed (e.g. balanced or skewed)."
      ],
      "metadata": {
        "id": "CK9V0dJAgbSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collections.Counter(y_train)"
      ],
      "metadata": {
        "id": "MHBWB2vc7ie1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we're relying more on richer encodings (in this case, word vectors), we won't perform as much preprocessing this time around. We'll stick with using the regular Keras **tokenizer** and just filter out numbers and certain symbols.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer<br><br>\n",
        "We'll also have the tokenizer limit itself to tokenizing only the most frequent 20,000 words. This way, the model will focus on the most frequent descriptive sentiment words."
      ],
      "metadata": {
        "id": "M0fM5vRygqfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(num_words=20000,\n",
        "                                               filters='0123456789!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\n",
        "                                               lower=True)"
      ],
      "metadata": {
        "id": "UI4FgHPa4p2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the vocabulary."
      ],
      "metadata": {
        "id": "gXQXX82chivF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tokenizer.fit_on_texts(train_reviews)"
      ],
      "metadata": {
        "id": "ayGLszCqzzoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to vectorize our reviews. In the [_Neural Network Foundations_](https://github.com/futuremojo/nlp-demystified/blob/main/notebooks/nlpdemystified_neural_networks_foundations.ipynb) notebook, we used the *texts_to_matrix* method to turn text into binary bags of words.<br><br>\n",
        "Here, we're going to use the *text_to_sequences* method to turn each review into a sequence of integers, with each integer representing its corresponding token.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences\n",
        "\n"
      ],
      "metadata": {
        "id": "v5zDD6RCBzxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "X_train = tokenizer.texts_to_sequences(train_reviews)"
      ],
      "metadata": {
        "id": "OAd431jp6Ydg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The first review in the training set, vectorized.\n",
        "print(X_train[0])"
      ],
      "metadata": {
        "id": "PPo0mNRB6YZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look up the corresponding tokens using the tokenizer's *index_word* dict. Here are the tokens corresponding to the first three integers from the first vectorized review."
      ],
      "metadata": {
        "id": "kH4I4SeoqVDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[tokenizer.index_word[x] for x in X_train[0][:3]]"
      ],
      "metadata": {
        "id": "jkOI7jYFqb44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also convert the integer sequence back to text using the *sequences_to_texts* method, and compare it against the original text.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#sequences_to_texts"
      ],
      "metadata": {
        "id": "ahgfyZhDrHDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Review excerpt reconstructed from integer sequence.\n",
        "tokenizer.sequences_to_texts([X_train[0]])[0][:300]"
      ],
      "metadata": {
        "id": "RPlvLdz8rGZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original review text.\n",
        "train_reviews.iloc[0][:300]"
      ],
      "metadata": {
        "id": "ZirHR4_OC87H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some models and situations require us to **pad** our sequences to the same length. While that's not the case here, it can still be beneficial to have all our inputs (and consequently, our batches) to be of uniform size to help with optimizations.<br><br>\n",
        "In this case, we'll make all our reviews 200 tokens in length (in practice, you can choose a number based on some analysis). So the reviews longer than 200 tokens will be truncated, while the reviews shorter than 200 will be padded with zeroes.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"
      ],
      "metadata": {
        "id": "bEqDNjactNk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_REVIEW_LEN = 200\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=MAX_REVIEW_LEN)"
      ],
      "metadata": {
        "id": "2lTEjMgDqIPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0])\n",
        "print(X_train[1])"
      ],
      "metadata": {
        "id": "d2tHPfAVv37k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our training set is prepared. We can now also vectorize and pad our validation set."
      ],
      "metadata": {
        "id": "nBAg3laKwDWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = tokenizer.texts_to_sequences(val_reviews)\n",
        "X_val = keras.preprocessing.sequence.pad_sequences(X_val, maxlen=MAX_REVIEW_LEN)"
      ],
      "metadata": {
        "id": "E9P72JJYHqKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to incorporate the Google News vectors (currently loaded into gensim) into our Keras model. What we'll do is create an embedding matrix that maps each tokenizer integer to its respective word vector.<br><br>\n",
        "For example, here's the index for the word \"good\" from the Keras tokenizer and the word vector for \"good\" from gensim. We want a matrix which maps the index to the vector.\n"
      ],
      "metadata": {
        "id": "P8dy9OWMwafM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index['good'])"
      ],
      "metadata": {
        "id": "5m7OW6-pwaJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part of the vector for the word 'good'.\n",
        "print(word_vectors['good'][:50])"
      ],
      "metadata": {
        "id": "3rOAy_IkHqfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create this embedding matrix by first initializing a matrix of zeros, then looping over every word in the tokenizer vocabulary and:\n",
        "1. Checking if the word has a corresponding vector in gensim.\n",
        "2. If it does, then copy the vector into the matrix row corresponding to the word's index."
      ],
      "metadata": {
        "id": "elzisAtxyO-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# + 1 to account for padding token.\n",
        "num_tokens = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Initialize a matrix of zeroes of size: vocabulary x embedding dimension.\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if word_vectors.has_index_for(word):\n",
        "    embedding_matrix[i] = word_vectors[word].copy()\n"
      ],
      "metadata": {
        "id": "Iw25Irs4J-JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick visual check.\n",
        "print(embedding_matrix[tokenizer.word_index['good']][:50])"
      ],
      "metadata": {
        "id": "uncvu4lcJ-OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're ready to build our first model using pretrained word vectors. The first layer we'll add is a Keras **embedding** layer which is essentially a trainable lookup table/matrix.<br>\n",
        "https://keras.io/api/layers/base_layer/#layer-class<br>\n",
        "https://keras.io/api/layers/core_layers/embedding/<br><br>\n",
        "In this case, we'll populate the **embedding** layer with the embedding matrix we created, and set *trainable* to True. This means we'll allow the learning algorithm training the classification model to adjust/fine-tune the word vectors as needed for greater performance. This corresponds to one of the scenarios we covered in the slides."
      ],
      "metadata": {
        "id": "CmzNcyFu0Noo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    input_length=MAX_REVIEW_LEN,\n",
        "    trainable=True\n",
        ")"
      ],
      "metadata": {
        "id": "ALs6rkTnJ-aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use a simple architecture for this model. Each training example is a sequence of *integers* which gets converted to a sequence of *vectors* (embeddings), but subsequent layers are expecting one vector per review. So we're inserting a **GlobalAveragePooling1D** layer after the embedding layer to average out all the word vectors into a single vector, before sending it further into the network. For classification, this can be pretty effective as a base model approach.<br>\n",
        "https://keras.io/api/layers/pooling_layers/global_average_pooling1d/<br>\n",
        "\n",
        "There was no science behind choosing 128 units in the first hidden layer and 64 units in the second hidden layer. The intuition was that the signal would be distilled from 300 dimensions down to 128 dimensions, then down to 64 dimensions before going to output.\n",
        "\n"
      ],
      "metadata": {
        "id": "OqRgEWpb2iuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "# This layer will output a sequence of 300-dimension *vectors*, one for each element in the input sequence.\n",
        "model.add(embedding_layer)\n",
        "\n",
        "# This layer will calculate an average of those vectors.\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "\n",
        "model.add(layers.Dense(128, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
        "model.add(layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
        "model.add(layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "1RrLd4gzbhVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example of what's going to happen under the hood to turn review text into a single vector for the dense layers:"
      ],
      "metadata": {
        "id": "dlmis9Oz4_jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review = \"fantastic papaya steak\"\n",
        "print(f\"Review: {review}\")\n",
        "\n",
        "review_sequence = tokenizer.texts_to_sequences([review])\n",
        "print(f\"Review as sequence of integers: {review_sequence}\")\n",
        "\n",
        "review_embeddings = embedding_layer(np.array(review_sequence))\n",
        "print(f\"Review embeddings shape: (Batch size: {review_embeddings.shape[0]}, \\\n",
        "Sequence length: {review_embeddings.shape[1]}, \\\n",
        "Embedding size: {review_embeddings.shape[2]})\")\n",
        "\n",
        "# How our document will be presented to the rest of the neural network.\n",
        "print(f\"Average of embeddings (shape): {np.mean(review_embeddings, axis=1).shape}\")"
      ],
      "metadata": {
        "id": "1r5K-Qqb1d-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we call the model's *summary* method, note how there are no params for the **GlobalAveragePooling1D** layer."
      ],
      "metadata": {
        "id": "546ouDEv41Ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "Pz31QQcb5nq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We won't use **early stopping** for this run. This way, we'll be able to compare metrics between the train and validation sets."
      ],
      "metadata": {
        "id": "AwGYMD028zhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, epochs=20, batch_size=512, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "oU6QBey79cis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_vs_val_performance(history):\n",
        "  training_losses = history.history['loss']\n",
        "  validation_losses = history.history['val_loss']\n",
        "\n",
        "  training_accuracy = history.history['accuracy']\n",
        "  validation_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(1, len(training_losses) + 1)\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  fig, (ax1, ax2) = plt.subplots(2)\n",
        "  fig.set_figheight(15)\n",
        "  fig.set_figwidth(15)\n",
        "  fig.tight_layout(pad=5.0)\n",
        "\n",
        "  # Plot training vs. validation loss.\n",
        "  ax1.plot(epochs, training_losses, 'bo', label='Training Loss')\n",
        "  ax1.plot(epochs, validation_losses, 'b', label='Validation Loss')\n",
        "  ax1.title.set_text('Training vs. Validation Loss')\n",
        "  ax1.set_xlabel('Epoch')\n",
        "  ax1.set_ylabel('Loss')\n",
        "  ax1.legend()\n",
        "\n",
        "  # PLot training vs. validation accuracy.\n",
        "  ax2.plot(epochs, training_accuracy, 'bo', label='Training Accuracy')\n",
        "  ax2.plot(epochs, validation_accuracy, 'b', label='Validation Accuracy')\n",
        "  ax2.title.set_text('Training vs. Validation Accuracy')\n",
        "  ax2.set_xlabel('Epoch')\n",
        "  ax2.set_ylabel('Accuracy')\n",
        "  ax2.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Neard5tEGwnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_vs_val_performance(history)"
      ],
      "metadata": {
        "id": "lbSk2qE1cEfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll initialize a new embedding layer and model and train for epochs equalling the point where we saw the validation loss diverge from the training loss.<br>\n",
        "\n",
        "**NOTE**: We need to initialize a new embedding layer here because we set the *learnable* parameter to **True** in the previous embedding layer. This means the previous embeddings were almost certainly updated by the learning algorithm. So we're re-training a new model now with the original embeddings."
      ],
      "metadata": {
        "id": "xdeEdLD6W91U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    input_length=MAX_REVIEW_LEN,\n",
        "    trainable=True\n",
        ")\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=512, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "id": "V_nWiYy6cEbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a trained model, let's try it on the test data. As we did with the training data, we'll:\n",
        "1. Replace the labels with 0 for negative sentiment, and 1 for positive.\n",
        "2. Convert the reviews into a sequence of integers and pad/truncate each review to a fixed length."
      ],
      "metadata": {
        "id": "_4X_sdScYPAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_test = pd.read_csv('yelp_review_polarity_csv/test.csv', names=['sentiment', 'review'])"
      ],
      "metadata": {
        "id": "AUsripS7SZdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_test['sentiment'].replace(to_replace=1, value=0, inplace=True)\n",
        "yelp_test['sentiment'].replace(to_replace=2, value=1, inplace=True)\n",
        "yelp_test.head()"
      ],
      "metadata": {
        "id": "bm4Day_jYbTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.array(yelp_test['sentiment'])\n",
        "print(y_test)"
      ],
      "metadata": {
        "id": "Qks0uXyzYbTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = tokenizer.texts_to_sequences(yelp_test['review'])"
      ],
      "metadata": {
        "id": "-UyBts6EYbTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=MAX_REVIEW_LEN)"
      ],
      "metadata": {
        "id": "loh2rh3WYbTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "yKmqmLUkYbTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad for a conceptually simple model where we average out a review's word vectors, run it through a few plain hidden layers, and out through a sigmoid function with no regularization and just using defaults for model components (e.g. optimizer settings).<br><br>\n",
        "We can now use the model for predictions."
      ],
      "metadata": {
        "id": "9GBMe-YKZCDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment(reviews):\n",
        "  seqs = tokenizer.texts_to_sequences(reviews)\n",
        "  seqs = keras.preprocessing.sequence.pad_sequences(seqs, maxlen=MAX_REVIEW_LEN)\n",
        "  return model.predict(seqs)\n"
      ],
      "metadata": {
        "id": "rNbNyr8PVJMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Real reviews from Google Reviews.\n",
        "pos_review = \"The best seafood joint in East Village San Diego!  Great lobster roll, great fish, great oysters, great bread, great cocktails, and such amazing service.  The atmosphere is top notch and the location is so much fun being located just a block away from Petco Park (San Diego Padres Stadium).\"\n",
        "neg_review = \"A thoroughly disappointing experience. When you book a Marriott you expect a certain standard. Albany falls way short. Room cleaning has to be booked 24 hours in advance but nobody thought to mention this at check in. The hotel is tired and needs a face-lift. The only bright light in a sea of mediocrity were the pancakes at breakfast. Sadly they weren't enough to save the experience. If you travel to Albany, then do yourself a big favour and book the Westin.\""
      ],
      "metadata": {
        "id": "Jy-4MWV1VJRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentiment([pos_review, neg_review]))"
      ],
      "metadata": {
        "id": "wH8b-Z3GVJU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training New Embeddings and a Model at the Same Time"
      ],
      "metadata": {
        "id": "EUX0AZqYbt1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this last model, rather than using pretrained embeddings, we'll start with a **random** embedding matrix and let the model come up with its own vectors simultaneously while fitting the training data.<br><br>\n",
        "We'll also use **early stopping**, but otherwise keep everything else the same."
      ],
      "metadata": {
        "id": "-ek2bnbCVJbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(0)\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "# The 'trainable' property is True by default.\n",
        "model.add(layers.Embedding(input_dim=num_tokens,\n",
        "                           output_dim=embedding_dim,\n",
        "                           input_length=MAX_REVIEW_LEN))\n",
        "\n",
        "\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "model.add(layers.Dense(128, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
        "model.add(layers.Dense(64, activation='relu', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
        "model.add(layers.Dense(1, activation='sigmoid', kernel_initializer=tf.keras.initializers.random_normal(seed=1)))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=512, validation_data=(X_val, y_val), callbacks=[es_callback])"
      ],
      "metadata": {
        "id": "wCy90PyxVJfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "5dL0GwLUVJio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like in this case, we get comparable performance between fine-tuning pretrained vectors and training embeddings from scratch as part of the model; likely because of the nature of the data and amount of it."
      ],
      "metadata": {
        "id": "n3Uy8zP-0WUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try This"
      ],
      "metadata": {
        "id": "Si2Nx7NSfl1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our first model, we used pretrained vectors in the **embedding layer** and set the *trainable* property to **True**, allowing the model to fine-tune the word vectors.<br><br>\n",
        "Instantiate the same model but this time, set the *trainable* property in the **embedding layer** to **False**. What happens to training performance? Does the training speed increase or decrease? What happens if you try to add some regularization like dropout?<br>\n",
        "\n",
        "Other things you can try to see what happens: reduce the number of units, use a slower learning rate, reduce the number of hidden layers, reduce vocabulary, reduce embedding dimensions, use regularization, use a shorter sequence length."
      ],
      "metadata": {
        "id": "FsQYls6a0zH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the embedding layer.\n",
        "\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Add layers.\n",
        "\n",
        "\n",
        "# Compile model.\n",
        "\n",
        "\n",
        "# Call fit.\n",
        "\n",
        "\n",
        "# Evaluate the model.\n"
      ],
      "metadata": {
        "id": "28u6ssTzflcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternative Static Embedding Algorithms"
      ],
      "metadata": {
        "id": "TgJM0Flw6Imq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe\n",
        "**GloVe (Global Vectors for Word Representation)** is another algorithm for creating static word vectors. You can read the original GloVe paper and download pretrained word vectors here:<br>\n",
        "https://nlp.stanford.edu/projects/glove/"
      ],
      "metadata": {
        "id": "Xcq27-hYKdMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Doc2Vec\n",
        "An algorithm which represents a document as a dense vector which addresses weaknesses of bag-of-words models.<br>\n",
        "https://arxiv.org/abs/1405.4053<br>\n",
        "https://radimrehurek.com/gensim/models/doc2vec.html<br>"
      ],
      "metadata": {
        "id": "331buv_FK4nQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fastText\n",
        "An alternative approach to creating embeddings. Instead of assigning a vector to each _word_ (e.g. a separate vector each for \"dog\" and \"dogs\"), a vector is assigned to each _subword_. For fastText, a subword is defined as a character n-gram.\n",
        "<br><br>\n",
        "So if n=3, then a word like \"hello\" would result in vectors for \"<he\", \"hel\", \"ell\", \"llo\", \"lo>\" (note that \"<\" and \">\" are special characters). The vector for \"hello\" would be the sum of all the above vectors. This helps deal with OOV situations because vectors can still be assigned to unseen words as long as the n-grams exist in the vocabulary.<br>\n",
        "https://fasttext.cc/<br>\n",
        "https://radimrehurek.com/gensim/models/fasttext.html\n",
        "<br><br>\n",
        "**We'll cover subword tokenization in greater detail later in the course.**"
      ],
      "metadata": {
        "id": "aRq8HklIg2Gu"
      }
    }
  ]
}